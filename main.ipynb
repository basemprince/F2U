{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from dataTransformation import labels4clients, distribute_data_labels4clients\n",
    "from gan_model import Discriminator, Generator, initialize_weights\n",
    "from network import Server, Worker\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import Logger\n",
    "from fid_score import *\n",
    "from inception import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 5\n",
    "CLASSES_PER_USER = 10\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "LEARNING_RATE = 2e-4\n",
    "B1 = 0.5\n",
    "B2 = 0.999\n",
    "BATCH_SIZE = 16\n",
    "NOISE_DIM = 128\n",
    "FID_BATCH_SIZE = 20\n",
    "NUM_UNIQUE_USERS = NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "logger = Logger(model_name='F2U',data_name='CIFAR10')\n",
    "dictionary = labels4clients(num_classes,CLASSES_PER_USER,NUM_WORKERS,NUM_UNIQUE_USERS,random_seed=False)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# trans_cifar = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.CIFAR10(root='./datasets/cifar/', train=True, download=True, transform=trans_cifar)\n",
    "dataset_test = datasets.CIFAR10(root='./datasets/cifar/', train=False, download=True, transform=trans_cifar)\n",
    "dataloader_one = torch.utils.data.DataLoader(dataset, shuffle = True,batch_size=BATCH_SIZE)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, shuffle = True,batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataloader_test:\n",
    "    test_imgs=img[0].to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data[0])\n",
    "# print(dataset.transforms(dataset.data[0],transforms.ToTensor()))\n",
    "# print(dataset.transforms(dataset.data[0],trans_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = trans_cifar(dataset.data[0]).cpu().detach().numpy()\n",
    "print(\"transformed shape:\", transformed.shape)\n",
    "plt.figure('normalized data')\n",
    "plt.hist(transformed.ravel(), bins=50, density=False)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data.shape)\n",
    "# print(type(dataset))\n",
    "# print(dataloader_one.dataset.data.shape)\n",
    "# x,_ = dataloader_one.dataset[0]\n",
    "# print(x.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np = np.empty((dataset.data.shape[0],dataset.data.shape[3],dataset.data.shape[1],dataset.data.shape[2]))\n",
    "print(\"train datatset shape:\",x_train_normalized_np.shape)\n",
    "for i in range(len(dataset)):\n",
    "    x_train_normalized_np[i] = trans_cifar(dataset.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('normalized data')\n",
    "bin_size = 60\n",
    "plt.hist(x_train_normalized_np[:][0].ravel(),color='r', bins=bin_size, density=False)\n",
    "plt.hist(x_train_normalized_np[:][1].ravel(),color='g', bins=bin_size, density=False)\n",
    "plt.hist(x_train_normalized_np[:][2].ravel(),color='b', bins=bin_size, density=False)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.asarray(dataset.data)\n",
    "y_train = np.asarray(dataset.targets)\n",
    "x_clinet_list, y_client_list = distribute_data_labels4clients(x_train_normalized_np,y_train,dictionary,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDist(y,num_classes,user_num):\n",
    "    ax = sns.countplot(x=y)\n",
    "    ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(x_clinet_list)):\n",
    "    print(len(y_client_list[i]))\n",
    "    getDist(y_client_list[i],num_classes,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "dev = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fic_model = InceptionV3().to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_server = Server(0,LEARNING_RATE,B1,B2)\n",
    "initialize_weights(main_server.generator)\n",
    "initialize_weights(main_server.global_disc)\n",
    "main_server.generator.train()\n",
    "workers = []\n",
    "workers_weights= []\n",
    "for i in range(NUM_WORKERS):\n",
    "    worker = Worker(i,LEARNING_RATE,B1,B2)\n",
    "    # x_clinet_list[i] = np.transpose(x_clinet_list[i],(0, 3, 1, 2))\n",
    "    worker.load_worker_data(x_clinet_list[i], y_client_list[i]) \n",
    "    initialize_weights(worker.discriminator)\n",
    "    worker.discriminator.train()\n",
    "    workers.append(worker)\n",
    "    workers_weights.append(worker.discriminator.state_dict())\n",
    "    \n",
    "# summary(main_server.generator,(128,1,1))\n",
    "# summary(workers[0].discriminator,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to make all the workers the same\n",
    "# workers_weights= []\n",
    "# for worker in workers:\n",
    "#     worker.discriminator = workers[-1].discriminator\n",
    "#     workers_weights.append(worker.discriminator.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "fixed_noise = torch.randn(36, NOISE_DIM, 1, 1).to(dev) # to use for generating output images\n",
    "\n",
    "worker_loaders = []\n",
    "\n",
    "for worker in workers:\n",
    "    # print(worker.x_data.shape)\n",
    "    worker_loaders.append([])\n",
    "    for batch_id, real in enumerate(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)):\n",
    "        worker_loaders[-1].append(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in worker_loaders:\n",
    "    plt.figure('normalized data')\n",
    "    plt.hist(worker[:][1].ravel(),color='r', bins=bin_size, density=False)\n",
    "    # plt.hist(x_train_normalized_np[:][1].ravel(),color='g', bins=bin_size, density=False)\n",
    "    # plt.hist(x_train_normalized_np[:][2].ravel(),color='b', bins=bin_size, density=False)\n",
    "    plt.xlabel(\"pixel values\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN archicture trial (trial == TRUE)\n",
    "if trial:\n",
    "    start = 0\n",
    "    end = start + NUM_EPOCHS\n",
    "    for epoch in range(start,end):\n",
    "        for i, data in enumerate(dataloader_one):\n",
    "            worker = workers[0]\n",
    "            noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "            fake = main_server.generator(noise)\n",
    "            real, _ = data\n",
    "\n",
    "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            \n",
    "            current_disc_real = worker.discriminator(real).reshape(-1)\n",
    "            # print('current discriminator real output', current_disc_real)\n",
    "            worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "            # print('worker loss_disc_real output', current_disc_real)\n",
    "            current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "            worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "            worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "            worker.discriminator.zero_grad()\n",
    "            worker.loss_disc.backward()\n",
    "            # total_norm_d =0\n",
    "            # for p in list(filter(lambda p: p.grad is not None, worker.discriminator.parameters())):\n",
    "            #     total_norm_d += p.grad.detach().data.norm(2).item()** 2\n",
    "            # total_norm_d = total_norm_d ** 0.5\n",
    "\n",
    "            worker.d_optimizer.step()\n",
    "\n",
    "            ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "            \n",
    "            output = worker.discriminator(fake).reshape(-1)\n",
    "            main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "            main_server.generator.zero_grad()\n",
    "            main_server.loss_gen.backward()\n",
    "\n",
    "            # total_norm_g =0\n",
    "            # for p in list(filter(lambda p: p.grad is not None, main_server.generator.parameters())):\n",
    "            #     total_norm_g += p.grad.detach().data.norm(2).item()** 2\n",
    "            # total_norm_g = total_norm_g ** 0.5\n",
    "\n",
    "            main_server.g_optimizer.step()\n",
    "\n",
    "\n",
    "            logger.log(worker.loss_disc.item(),main_server.loss_gen.item(),worker.loss_disc_real, worker.loss_disc_fake,epoch,i,len(dataloader_one))\n",
    "\n",
    "            # Print loss\n",
    "            if i % 100 == 0:    \n",
    "                fid_z = torch.randn(FID_BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "                gen_imgs = main_server.generator(fid_z.detach())\n",
    "                mu_gen, sigma_gen = calculate_activation_statistics(gen_imgs, fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "                mu_test, sigma_test = calculate_activation_statistics(test_imgs[:FID_BATCH_SIZE], fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "                fid = calculate_frechet_distance(mu_gen, sigma_gen, mu_test, sigma_test)\n",
    "                logger.log_fid(fid,epoch,i,len(dataloader_one))\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{end}] Batch {i}/{len(dataloader_one)} \\\n",
    "                    Loss D: {worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}, FID Score: {fid:.1f}\"\n",
    "                )\n",
    "\n",
    "            if i% 500 == 0:\n",
    "                with torch.no_grad():\n",
    "                    fake = main_server.generator(fixed_noise)\n",
    "                    logger.log_images(fake,len(fake), epoch, i, len(dataloader_one))\n",
    "        if epoch % 50 == 0 and epoch !=0:\n",
    "            logger.save_models(main_server,workers,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop for F2U (trial == FALSE)\n",
    "fed_avg = False\n",
    "if not trial:\n",
    "    start = 0\n",
    "    end = start + NUM_EPOCHS\n",
    "    end = NUM_EPOCHS\n",
    "    worker_chosen_counter = [0 for i in range(len(workers))]\n",
    "\n",
    "    for epoch in range(start,end):\n",
    "        for batch_id in range(len(worker_loaders[0])):\n",
    "\n",
    "            highest_loss = 0\n",
    "            lowest_loss = math.inf\n",
    "            chosen_discriminator = None\n",
    "            noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "            fake = main_server.generator(noise)\n",
    "\n",
    "            for worker_id, worker in enumerate(workers):\n",
    "                current_worker_real = worker_loaders[0][batch_id].float().to(dev)\n",
    "                # print(real.shape)\n",
    "\n",
    "                ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "                current_disc_real = worker.discriminator(current_worker_real).reshape(-1)\n",
    "                worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "                current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "                worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "                worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "                worker.discriminator.zero_grad()\n",
    "                worker.loss_disc.backward()\n",
    "                worker.d_optimizer.step()\n",
    "\n",
    "                workers_weights[worker_id] = worker.discriminator.state_dict()\n",
    "                # print(worker.loss_disc_fake, i)\n",
    "                if highest_loss < worker.loss_disc_fake:\n",
    "                    highest_loss = worker.loss_disc_fake\n",
    "                    chosen_discriminator = worker_id\n",
    "                # if lowest_loss > worker.loss_disc_fake:\n",
    "                #     lowest_loss = worker.loss_disc_fake\n",
    "                #     chosen_discriminator = worker_id\n",
    "            # print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item():.4f}\")\n",
    "            \n",
    "            ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "            chosen_worker = workers[chosen_discriminator]\n",
    "            worker_chosen_counter[chosen_discriminator]+=1\n",
    "            if fed_avg:\n",
    "                avg_w = main_server.fed_average(workers_weights)\n",
    "                main_server.global_disc.load_state_dict(avg_w)\n",
    "                output = main_server.global_disc(fake).reshape(-1)\n",
    "            else:\n",
    "                output = chosen_worker.discriminator(fake).reshape(-1)\n",
    "            main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "            main_server.generator.zero_grad()\n",
    "            main_server.loss_gen.backward()\n",
    "            main_server.g_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logger.log_workers(workers,epoch,batch_id,len(worker_loaders[0]))\n",
    "                # logger.log(chosen_worker.loss_disc.item(),main_server.loss_gen.item(),chosen_worker.loss_disc_real, chosen_worker.loss_disc_fake,epoch,batch_id,len(worker_loaders[0]))\n",
    "\n",
    "            # Print loss\n",
    "            if batch_id % 100 == 0:\n",
    "                fid_z = torch.randn(FID_BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "                gen_imgs = main_server.generator(fid_z.detach())\n",
    "                mu_gen, sigma_gen = calculate_activation_statistics(gen_imgs, fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "                mu_test, sigma_test = calculate_activation_statistics(test_imgs[:FID_BATCH_SIZE], fic_model, batch_size=FID_BATCH_SIZE)\n",
    "                fid = calculate_frechet_distance(mu_gen, sigma_gen, mu_test, sigma_test)\n",
    "                logger.log_fid(fid,epoch,batch_id,len(worker_loaders[0]))\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(worker_loaders[0])} \\\n",
    "                    Loss D: {chosen_worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}, FID Score: {fid:.1f}\"\n",
    "                )\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            fake = main_server.generator(fixed_noise)\n",
    "            logger.log_images(fake,len(fake), epoch, batch_id, len(worker_loaders[0]))\n",
    "\n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            plt.bar(range(len(worker_chosen_counter)),worker_chosen_counter)\n",
    "            plt.xlabel('worker number')\n",
    "            plt.ylabel('chosen counter')\n",
    "            plt.show()\n",
    "        if epoch % 50 == 0 and epoch != 0:\n",
    "            logger.save_models(main_server,workers,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(worker_chosen_counter)),worker_chosen_counter)\n",
    "plt.xlabel('worker number')\n",
    "plt.ylabel('chosen counter')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
