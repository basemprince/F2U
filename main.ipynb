{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from dataTransformation import labels4clients, distribute_data_labels4clients\n",
    "from gan_model import Discriminator, Generator, initialize_weights\n",
    "from network import Server, Worker\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import Logger\n",
    "from fid_score import *\n",
    "from inception import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 64\n",
    "NOISE_DIM = 128\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0], 1: [0], 2: [0], 3: [0], 4: [0], 5: [0], 6: [0], 7: [0], 8: [0], 9: [0]}\n"
     ]
    }
   ],
   "source": [
    "num_workers = 1\n",
    "num_unique_users = num_workers\n",
    "num_classes = 10\n",
    "classes_per_user = 10\n",
    "\n",
    "logger = Logger(model_name='F2U',data_name='CIFAR10')\n",
    "dictionary = labels4clients(num_classes,classes_per_user,num_workers,num_unique_users,False)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# trans_cifar = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.CIFAR10(root='./datasets/cifar/', train=True, download=True, transform=trans_cifar)\n",
    "dataset_test = datasets.CIFAR10(root='./datasets/cifar/', train=False, download=True, transform=trans_cifar)\n",
    "dataloader_one = torch.utils.data.DataLoader(dataset, shuffle = True,batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = []\n",
    "for i,t in enumerate(dataset_test):\n",
    "    test_imgs.append(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data[0])\n",
    "# print(dataset.transforms(dataset.data[0],transforms.ToTensor()))\n",
    "# print(dataset.transforms(dataset.data[0],trans_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "print(normalized)\n",
    "trans_cifar(dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data.shape)\n",
    "# print(type(dataset))\n",
    "# print(dataloader_one.dataset.data.shape)\n",
    "# x,_ = dataloader_one.dataset[0]\n",
    "# print(x.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fic_model = InceptionV3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np = np.empty((dataset.data.shape[0],dataset.data.shape[3],dataset.data.shape[1],dataset.data.shape[2]))\n",
    "for i in range(len(dataset)):\n",
    "    x_train_normalized_np[i] = trans_cifar(dataset.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(dataset.data)\n",
    "y_train = np.asarray(dataset.targets)\n",
    "x_clinet_list, y_client_list = distribute_data_labels4clients(x_train_normalized_np,y_train,dictionary,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDist(y,num_classes,user_num):\n",
    "    ax = sns.countplot(x=y)\n",
    "    ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(x_clinet_list)):\n",
    "    print(len(y_client_list[i]))\n",
    "    getDist(y_client_list[i],num_classes,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "dev = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG = Generator().to(dev)\n",
    "# netD = Discriminator().to(dev)\n",
    "# summary(netG,(128,1,1))\n",
    "# summary(netD,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_server = Server(0,LEARNING_RATE)\n",
    "main_server.generator.train()\n",
    "workers = []\n",
    "for i in range(num_workers):\n",
    "    worker = Worker(i,LEARNING_RATE)\n",
    "    # x_clinet_list[i] = np.transpose(x_clinet_list[i],(0, 3, 1, 2))\n",
    "    worker.load_worker_data(x_clinet_list[i], y_client_list[i])\n",
    "    worker.discriminator.train()\n",
    "    workers.append(worker)\n",
    "    \n",
    "# summary(main_server.generator,(128,1,1))\n",
    "# summary(workers[0].discriminator,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "fixed_noise = torch.randn(36, NOISE_DIM, 1, 1).to(dev)\n",
    "\n",
    "worker_loaders = []\n",
    "\n",
    "for worker in workers:\n",
    "    # print(worker.x_data.shape)\n",
    "    worker_loaders.append([])\n",
    "    for batch_id, real in enumerate(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)):\n",
    "        worker_loaders[-1].append(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN archicture trial\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, data in enumerate(dataloader_one):\n",
    "        worker = workers[0]\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "        fake = main_server.generator(noise)\n",
    "        real, _ = data\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        worker.discriminator.zero_grad()\n",
    "        current_disc_real = worker.discriminator(real).reshape(-1)\n",
    "        # print('current discriminator real output', current_disc_real)\n",
    "        worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real)*0.9)\n",
    "        # print('worker loss_disc_real output', current_disc_real)\n",
    "        current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "        worker.loss_disc_fake = criterion(current_disc_fake, torch.ones_like(current_disc_fake)*0.1)\n",
    "        worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "        \n",
    "        worker.loss_disc.backward()\n",
    "        # total_norm_d =0\n",
    "        # for p in list(filter(lambda p: p.grad is not None, worker.discriminator.parameters())):\n",
    "        #     total_norm_d += p.grad.detach().data.norm(2).item()** 2\n",
    "        # total_norm_d = total_norm_d ** 0.5\n",
    "\n",
    "        worker.d_optimizer.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        main_server.generator.zero_grad()\n",
    "        output = worker.discriminator(fake).reshape(-1)\n",
    "        main_server.loss_gen = criterion(output, torch.ones_like(output)*0.9)\n",
    "        \n",
    "        main_server.loss_gen.backward()\n",
    "\n",
    "        # total_norm_g =0\n",
    "        # for p in list(filter(lambda p: p.grad is not None, main_server.generator.parameters())):\n",
    "        #     total_norm_g += p.grad.detach().data.norm(2).item()** 2\n",
    "        # total_norm_g = total_norm_g ** 0.5\n",
    "\n",
    "        main_server.g_optimizer.step()\n",
    "\n",
    "\n",
    "        logger.log(worker.loss_disc.item(),main_server.loss_gen.item(),worker.loss_disc_real, worker.loss_disc_fake,epoch,i,len(dataloader_one))\n",
    "\n",
    "        # Print loss\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {i}/{len(dataloader_one)} \\\n",
    "                 Loss D: {worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}\"\n",
    "            )\n",
    "            fid_z = Variable(torch.cuda.FloatTensor(np.random.normal(0, 1, (BATCH_SIZE, NOISE_DIM))))\n",
    "            gen_imgs = main_server.generator(fid_z)\n",
    "            mu_gen, sigma_gen = calculate_activation_statistics(gen_imgs, fic_model, batch_size=BATCH_SIZE)\n",
    "            mu_test, sigma_test = calculate_activation_statistics(test_imgs[:BATCH_SIZE], fic_model, batch_size=BATCH_SIZE)\n",
    "            fid = calculate_frechet_distance(mu_gen, sigma_gen, mu_test, sigma_test)\n",
    "            print(\"FID Score: \", fid)\n",
    "\n",
    "        if i% 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = main_server.generator(fixed_noise)\n",
    "                logger.log_images(fake,len(fake), epoch, i, len(dataloader_one))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop for F2U\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_id in range(len(worker_loaders[0])):\n",
    "\n",
    "        highest_loss = 0\n",
    "        chosen_discriminator = None\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "        fake = main_server.generator(noise)\n",
    "\n",
    "        for worker_id, worker in enumerate(workers):\n",
    "            current_worker_real = worker_loaders[worker_id][batch_id].float().to(dev)\n",
    "            # print(real.shape)\n",
    "\n",
    "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            current_disc_real = worker.discriminator(current_worker_real).reshape(-1)\n",
    "            worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "            current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "            worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "            worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "            worker.discriminator.zero_grad()\n",
    "            worker.loss_disc.backward()\n",
    "            worker.d_optimizer.step()\n",
    "            # print(worker.loss_disc_fake, i)\n",
    "            if highest_loss < worker.loss_disc_fake:\n",
    "                highest_loss = worker.loss_disc_fake\n",
    "                chosen_discriminator = worker_id\n",
    "        # print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item():.4f}\")\n",
    "        chosen_worker = workers[chosen_discriminator]\n",
    "        \n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = chosen_worker.discriminator(fake).reshape(-1)\n",
    "        main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "        main_server.generator.zero_grad()\n",
    "        main_server.loss_gen.backward()\n",
    "        main_server.g_optimizer.step()\n",
    "\n",
    "        logger.log(chosen_worker.loss_disc.item(),main_server.loss_gen.item(),chosen_worker.loss_disc_real, chosen_worker.loss_disc_fake,epoch,batch_id,len(worker_loaders[0]))\n",
    "\n",
    "        # Print loss\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(worker_loaders[0])} \\\n",
    "                 Loss D: {chosen_worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}\"\n",
    "            )\n",
    "        if batch_id% 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = main_server.generator(fixed_noise)\n",
    "                logger.log_images(fake,len(fake), epoch, batch_id, len(worker_loaders[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = []\n",
    "\n",
    "# for worker in workers:\n",
    "#     # print(worker.x_data.shape)\n",
    "#     dataloaders.append(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE))\n",
    "\n",
    "# i = iter(dataloaders[0])\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# NOISE_DIM = 128\n",
    "# fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(dev)\n",
    "# writer_real = SummaryWriter(f\"logs/real\")\n",
    "# writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "# step = 0\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     highest_loss = 0\n",
    "#     chosen_discriminator = None\n",
    "#     for i, worker in enumerate(workers):\n",
    "#         print(worker.x_data.shape)\n",
    "#         dataloader = DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)\n",
    "#         for batch_id, real in enumerate(dataloader):\n",
    "#             real = real.float().to(dev)\n",
    "#             # print(real.shape)\n",
    "#             # print(real)\n",
    "#             noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "#             fake = main_server.generator(noise)\n",
    "\n",
    "#             ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "#             disc_real = worker.discriminator(real).reshape(-1)\n",
    "#             worker.loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "#             disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "#             worker.loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "#             loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "#             worker.discriminator.zero_grad()\n",
    "#             loss_disc.backward()\n",
    "#             worker.d_optimizer.step()\n",
    "#             if batch_id % 20 == 0:\n",
    "#                 print(\n",
    "#                     f\"Worker: {i} Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(dataloader)} \\\n",
    "#                         Loss D: {loss_disc:.4f}\"\n",
    "#                 )\n",
    "#         # print(worker.loss_disc_fake, i)\n",
    "#         if highest_loss < worker.loss_disc_fake:\n",
    "#             highest_loss = worker.loss_disc_fake\n",
    "#             chosen_discriminator = i\n",
    "#         print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item()}\")\n",
    "#     dataloader = DataLoader(dataset=workers[chosen_discriminator].x_data,batch_size=BATCH_SIZE)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "    # output = disc(fake).reshape(-1)\n",
    "    # loss_gen = criterion(output, torch.ones_like(output))\n",
    "    # gen.zero_grad()\n",
    "    # loss_gen.backward()\n",
    "    # opt_gen.step()\n",
    "\n",
    "    # for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    #     real = real.to(device)\n",
    "    #     noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "    #     fake = gen(noise)\n",
    "\n",
    "    #     ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "    #     disc_real = disc(real).reshape(-1)\n",
    "    #     loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "    #     disc_fake = disc(fake.detach()).reshape(-1)\n",
    "    #     loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "    #     loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "    #     disc.zero_grad()\n",
    "    #     loss_disc.backward()\n",
    "    #     opt_disc.step()\n",
    "\n",
    "    #     ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "    #     output = disc(fake).reshape(-1)\n",
    "    #     loss_gen = criterion(output, torch.ones_like(output))\n",
    "    #     gen.zero_grad()\n",
    "    #     loss_gen.backward()\n",
    "    #     opt_gen.step()\n",
    "\n",
    "    #     # Print losses occasionally and print to tensorboard\n",
    "    #     if batch_idx % 100 == 0:\n",
    "    #         print(\n",
    "    #             f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "    #               Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "    #         )\n",
    "\n",
    "    #         with torch.no_grad():\n",
    "    #             fake = gen(fixed_noise)\n",
    "    #             # take out (up to) 32 examples\n",
    "    #             img_grid_real = torchvision.utils.make_grid(\n",
    "    #                 real[:32], normalize=True\n",
    "    #             )\n",
    "    #             img_grid_fake = torchvision.utils.make_grid(\n",
    "    #                 fake[:32], normalize=True\n",
    "    #             )\n",
    "\n",
    "    #             writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "    #             writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "    #         step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
