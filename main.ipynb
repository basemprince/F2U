{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from dataTransformation import labels4clients, distribute_data_labels4clients\n",
    "from gan_model import Discriminator, Generator, initialize_weights\n",
    "from network import Server, Worker\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 64\n",
    "NOISE_DIM = 128\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0], 1: [0], 2: [0], 3: [0], 4: [0], 5: [0], 6: [0], 7: [0], 8: [0], 9: [0]}\n"
     ]
    }
   ],
   "source": [
    "num_workers = 1\n",
    "num_unique_users = num_workers\n",
    "num_classes = 10\n",
    "classes_per_user = 10\n",
    "\n",
    "logger = Logger(model_name='F2U',data_name='CIFAR10')\n",
    "dictionary = labels4clients(num_classes,classes_per_user,num_workers,num_unique_users,False)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.CIFAR10(root='./datasets/cifar/', train=True, download=True, transform=trans_cifar)\n",
    "dataset_test = datasets.CIFAR10(root='./datasets/cifar/', train=False, download=True, transform=trans_cifar)\n",
    "dataloader_one = torch.utils.data.DataLoader(dataset, shuffle = True,batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 59  62  63]\n",
      "  [ 43  46  45]\n",
      "  [ 50  48  43]\n",
      "  ...\n",
      "  [158 132 108]\n",
      "  [152 125 102]\n",
      "  [148 124 103]]\n",
      "\n",
      " [[ 16  20  20]\n",
      "  [  0   0   0]\n",
      "  [ 18   8   0]\n",
      "  ...\n",
      "  [123  88  55]\n",
      "  [119  83  50]\n",
      "  [122  87  57]]\n",
      "\n",
      " [[ 25  24  21]\n",
      "  [ 16   7   0]\n",
      "  [ 49  27   8]\n",
      "  ...\n",
      "  [118  84  50]\n",
      "  [120  84  50]\n",
      "  [109  73  42]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[208 170  96]\n",
      "  [201 153  34]\n",
      "  [198 161  26]\n",
      "  ...\n",
      "  [160 133  70]\n",
      "  [ 56  31   7]\n",
      "  [ 53  34  20]]\n",
      "\n",
      " [[180 139  96]\n",
      "  [173 123  42]\n",
      "  [186 144  30]\n",
      "  ...\n",
      "  [184 148  94]\n",
      "  [ 97  62  34]\n",
      "  [ 83  53  34]]\n",
      "\n",
      " [[177 144 116]\n",
      "  [168 129  94]\n",
      "  [179 142  87]\n",
      "  ...\n",
      "  [216 184 140]\n",
      "  [151 118  84]\n",
      "  [123  92  72]]]\n",
      "(tensor([[[-0.5373, -0.6627, -0.6078,  ...,  0.2392,  0.1922,  0.1608],\n",
      "         [-0.8745, -1.0000, -0.8588,  ..., -0.0353, -0.0667, -0.0431],\n",
      "         [-0.8039, -0.8745, -0.6157,  ..., -0.0745, -0.0588, -0.1451],\n",
      "         ...,\n",
      "         [ 0.6314,  0.5765,  0.5529,  ...,  0.2549, -0.5608, -0.5843],\n",
      "         [ 0.4118,  0.3569,  0.4588,  ...,  0.4431, -0.2392, -0.3490],\n",
      "         [ 0.3882,  0.3176,  0.4039,  ...,  0.6941,  0.1843, -0.0353]],\n",
      "\n",
      "        [[-0.5137, -0.6392, -0.6235,  ...,  0.0353, -0.0196, -0.0275],\n",
      "         [-0.8431, -1.0000, -0.9373,  ..., -0.3098, -0.3490, -0.3176],\n",
      "         [-0.8118, -0.9451, -0.7882,  ..., -0.3412, -0.3412, -0.4275],\n",
      "         ...,\n",
      "         [ 0.3333,  0.2000,  0.2627,  ...,  0.0431, -0.7569, -0.7333],\n",
      "         [ 0.0902, -0.0353,  0.1294,  ...,  0.1608, -0.5137, -0.5843],\n",
      "         [ 0.1294,  0.0118,  0.1137,  ...,  0.4431, -0.0745, -0.2784]],\n",
      "\n",
      "        [[-0.5059, -0.6471, -0.6627,  ..., -0.1529, -0.2000, -0.1922],\n",
      "         [-0.8431, -1.0000, -1.0000,  ..., -0.5686, -0.6078, -0.5529],\n",
      "         [-0.8353, -1.0000, -0.9373,  ..., -0.6078, -0.6078, -0.6706],\n",
      "         ...,\n",
      "         [-0.2471, -0.7333, -0.7961,  ..., -0.4510, -0.9451, -0.8431],\n",
      "         [-0.2471, -0.6706, -0.7647,  ..., -0.2627, -0.7333, -0.7333],\n",
      "         [-0.0902, -0.2627, -0.3176,  ...,  0.0980, -0.3412, -0.4353]]]), ToTensor())\n",
      "(tensor([[[-0.5373, -0.6627, -0.6078,  ...,  0.2392,  0.1922,  0.1608],\n",
      "         [-0.8745, -1.0000, -0.8588,  ..., -0.0353, -0.0667, -0.0431],\n",
      "         [-0.8039, -0.8745, -0.6157,  ..., -0.0745, -0.0588, -0.1451],\n",
      "         ...,\n",
      "         [ 0.6314,  0.5765,  0.5529,  ...,  0.2549, -0.5608, -0.5843],\n",
      "         [ 0.4118,  0.3569,  0.4588,  ...,  0.4431, -0.2392, -0.3490],\n",
      "         [ 0.3882,  0.3176,  0.4039,  ...,  0.6941,  0.1843, -0.0353]],\n",
      "\n",
      "        [[-0.5137, -0.6392, -0.6235,  ...,  0.0353, -0.0196, -0.0275],\n",
      "         [-0.8431, -1.0000, -0.9373,  ..., -0.3098, -0.3490, -0.3176],\n",
      "         [-0.8118, -0.9451, -0.7882,  ..., -0.3412, -0.3412, -0.4275],\n",
      "         ...,\n",
      "         [ 0.3333,  0.2000,  0.2627,  ...,  0.0431, -0.7569, -0.7333],\n",
      "         [ 0.0902, -0.0353,  0.1294,  ...,  0.1608, -0.5137, -0.5843],\n",
      "         [ 0.1294,  0.0118,  0.1137,  ...,  0.4431, -0.0745, -0.2784]],\n",
      "\n",
      "        [[-0.5059, -0.6471, -0.6627,  ..., -0.1529, -0.2000, -0.1922],\n",
      "         [-0.8431, -1.0000, -1.0000,  ..., -0.5686, -0.6078, -0.5529],\n",
      "         [-0.8353, -1.0000, -0.9373,  ..., -0.6078, -0.6078, -0.6706],\n",
      "         ...,\n",
      "         [-0.2471, -0.7333, -0.7961,  ..., -0.4510, -0.9451, -0.8431],\n",
      "         [-0.2471, -0.6706, -0.7647,  ..., -0.2627, -0.7333, -0.7333],\n",
      "         [-0.0902, -0.2627, -0.3176,  ...,  0.0980, -0.3412, -0.4353]]]), Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data[0])\n",
    "print(dataset.transforms(dataset.data[0],transforms.ToTensor()))\n",
    "print(dataset.transforms(dataset.data[0],trans_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5373, -0.6627, -0.6078,  ...,  0.2392,  0.1922,  0.1608],\n",
       "         [-0.8745, -1.0000, -0.8588,  ..., -0.0353, -0.0667, -0.0431],\n",
       "         [-0.8039, -0.8745, -0.6157,  ..., -0.0745, -0.0588, -0.1451],\n",
       "         ...,\n",
       "         [ 0.6314,  0.5765,  0.5529,  ...,  0.2549, -0.5608, -0.5843],\n",
       "         [ 0.4118,  0.3569,  0.4588,  ...,  0.4431, -0.2392, -0.3490],\n",
       "         [ 0.3882,  0.3176,  0.4039,  ...,  0.6941,  0.1843, -0.0353]],\n",
       "\n",
       "        [[-0.5137, -0.6392, -0.6235,  ...,  0.0353, -0.0196, -0.0275],\n",
       "         [-0.8431, -1.0000, -0.9373,  ..., -0.3098, -0.3490, -0.3176],\n",
       "         [-0.8118, -0.9451, -0.7882,  ..., -0.3412, -0.3412, -0.4275],\n",
       "         ...,\n",
       "         [ 0.3333,  0.2000,  0.2627,  ...,  0.0431, -0.7569, -0.7333],\n",
       "         [ 0.0902, -0.0353,  0.1294,  ...,  0.1608, -0.5137, -0.5843],\n",
       "         [ 0.1294,  0.0118,  0.1137,  ...,  0.4431, -0.0745, -0.2784]],\n",
       "\n",
       "        [[-0.5059, -0.6471, -0.6627,  ..., -0.1529, -0.2000, -0.1922],\n",
       "         [-0.8431, -1.0000, -1.0000,  ..., -0.5686, -0.6078, -0.5529],\n",
       "         [-0.8353, -1.0000, -0.9373,  ..., -0.6078, -0.6078, -0.6706],\n",
       "         ...,\n",
       "         [-0.2471, -0.7333, -0.7961,  ..., -0.4510, -0.9451, -0.8431],\n",
       "         [-0.2471, -0.6706, -0.7647,  ..., -0.2627, -0.7333, -0.7333],\n",
       "         [-0.0902, -0.2627, -0.3176,  ...,  0.0980, -0.3412, -0.4353]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "print(normalized)\n",
    "trans_cifar(dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data.shape)\n",
    "# print(type(dataset))\n",
    "# print(dataloader_one.dataset.data.shape)\n",
    "# x,_ = dataloader_one.dataset[0]\n",
    "# print(x.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np = np.empty((dataset.data.shape[0],dataset.data.shape[3],dataset.data.shape[1],dataset.data.shape[2]))\n",
    "for i in range(len(dataset)):\n",
    "    x_train_normalized_np[i] = trans_cifar(dataset.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(dataset.data)\n",
    "y_train = np.asarray(dataset.targets)\n",
    "x_clinet_list, y_client_list = distribute_data_labels4clients(x_train_normalized_np,y_train,dictionary,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDist(y,num_classes,user_num):\n",
    "    ax = sns.countplot(x=y)\n",
    "    ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGzCAYAAADOnwhmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5qUlEQVR4nO3de1hVdd7//xegG5CjB+TggfCQqCmOlkrmMUZScnI81m1FovbV0EQKHe/KYw5lt5kpapap0+SUVlpqqXjCMfEQieMps7I0FSgT8AgK6/fHXOyfO9QAwYWu5+O61nW5P+u9P+v9cVO8XHutvZ0MwzAEAABgYc5mNwAAAGA2AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhGAcnXlyhWNHTtW9erVk7Ozs3r37l3qOe666y499dRT5d5bRfrxxx/l5OSkxYsXm91KhXrvvfcUGhqqqlWrytfX1+x2gHJDIAIqwPfff6//9//+nxo0aCA3Nzd5e3urQ4cOmjVrli5evGh2e5KkuXPnVsgv73fffVevvfaa+vXrpyVLlmjMmDHlfozrOXnypCZNmqT09PRbdkwr+eabb/TUU0+pYcOGevvtt7VgwYIKP2Z2draefvpp+fn5ycPDQ127dtXXX39d4ceF9VQxuwHgTrNmzRr1799frq6uevLJJ3XPPfcoPz9f27ZtU0JCgg4cOHBLfpH8kblz56pWrVrlfiZm06ZNqlOnjmbOnFmu85bEyZMnNXnyZN11111q1arVLT/+nW7Lli0qLCzUrFmz1KhRowo/XmFhoaKiorR3714lJCSoVq1amjt3rrp06aK0tDQ1bty4wnuAdRCIgHJ09OhRPfroowoODtamTZsUGBho3xcbG6vvvvtOa9asMbHDipeVlcVbKXeorKwsSSrX1/fChQuqVq3aNfd99NFH2r59u5YvX65+/fpJkgYMGKC7775bEydO1NKlS8utD0AGgHIzfPhwQ5Lx5Zdflqj+8uXLxpQpU4wGDRoYNpvNCA4ONsaPH29cunTJoU6SMXHixGLPDw4ONqKjo+2PFy1aZEgytm3bZowZM8aoVauWUa1aNaN3795GVlaWw/MkOWydO3e+Ya/nzp0z4uPjjbp16xo2m824++67jddee80oLCw0DMMwjh49WmxOScbmzZuvO2dhYaExdepUo06dOoa7u7vRpUsXY//+/cXWdfr0aeO5554z7rnnHsPDw8Pw8vIyHnroISM9Pd1es3nz5msef9GiRYZhGMbWrVuNfv36GfXq1TNsNptRt25dIy4uzrhw4cIN113kzJkzRlxcnBEcHGzYbDajTp06xhNPPGH88ssvDusvOp5hGMbevXuN6OhoIyQkxHB1dTX8/f2NwYMHG7/++qvD3Lm5ucbo0aPtc/v5+RkRERFGWlqavebbb781+vTpY/j7+xuurq5GnTp1jIEDBxrZ2dkOc7333ntG69atDTc3N6N69erGwIEDjWPHjjnUlHSuq13rZ+bqn8mkpCSjWbNmhs1mMwIDA41nnnnGOHPmjMMcnTt3Npo3b2589dVXRseOHQ13d3dj9OjR1z1m//79DX9/f6OgoMBh/OmnnzaqVatW7L8T4GZwhggoR6tWrVKDBg10//33l6h+6NChWrJkifr166fnnntOO3fuVGJiog4dOqQVK1aUuY9Ro0apevXqmjhxon788Ue98cYbGjlypD788ENJ0htvvKFRo0bJ09NTL7zwgiTJ39//uvMZhqG//OUv2rx5s4YMGaJWrVpp3bp1SkhI0IkTJzRz5kz5+fnpvffe07Rp03Tu3DklJiZKkpo2bXrdeSdMmKCXX35ZPXv2VM+ePfX111+re/fuys/Pd6j74YcftHLlSvXv318hISHKzMzUW2+9pc6dO+vgwYMKCgpS06ZNNWXKFE2YMEFPP/20OnbsKEn212L58uW6cOGCRowYoZo1a2rXrl2aPXu2fv75Zy1fvvyGf5/nzp1Tx44ddejQIcXExKh169b69ddf9dlnn+nnn39WrVq1rvm85ORk/fDDDxo8eLACAgLsb5ceOHBAO3bskJOTkyRp+PDh+uijjzRy5Eg1a9ZMp0+f1rZt23To0CG1bt1a+fn5ioyMVF5enkaNGqWAgACdOHFCq1evVnZ2tnx8fCRJ06ZN00svvaQBAwZo6NCh+uWXXzR79mx16tRJe/bska+vb4nn+r033nhD//jHP7RixQrNmzdPnp6eatmypSRp0qRJmjx5siIiIjRixAgdPnxY8+bN0+7du/Xll1+qatWq9nlOnz6tHj166NFHH9Xjjz9+w5+7PXv2qHXr1nJ2drzctW3btlqwYIG+/fZbtWjR4oavHVBiZicy4E6Rk5NjSDIeeeSREtWnp6cbkoyhQ4c6jD///POGJGPTpk32MZXyDFFERIT9zI1hGMaYMWMMFxcXhzMAzZs3/8OzQkVWrlxpSDJefvllh/F+/foZTk5OxnfffWcfKzoL8EeysrIMm81mREVFOfT6v//7v4Ykh3VdunSp2FmCo0ePGq6ursaUKVPsY7t37y52lqbItc4EJSYmGk5OTsZPP/10w14nTJhgSDI++eSTYvt+f4bs6mNf65j/+te/DEnG1q1b7WM+Pj5GbGzsdY+/Z88eQ5KxfPny69b8+OOPhouLizFt2jSH8X379hlVqlSxj5dkruuZOHGiIcl+Vsww/v/XsXv37g6v0Zw5cwxJxrvvvmsf69y5syHJmD9/fomO5+HhYcTExBQbX7NmjSHJWLt2banXAFwPd5kB5SQ3N1eS5OXlVaL6zz//XJIUHx/vMP7cc89J0k1da/T000/bzz5IUseOHVVQUKCffvqpTPN9/vnncnFx0bPPPlusV8Mw9MUXX5R6zg0bNig/P1+jRo1y6DUuLq5Yraurq/0sQUFBgU6fPi1PT081adKkxHccubu72/98/vx5/frrr7r//vtlGIb27Nlzw+d+/PHHCgsL01//+tdi+67u/UbHvHTpkn799Ve1b99ekhz69vX11c6dO3Xy5MlrzlN01mbdunW6cOHCNWs++eQTFRYWasCAAfr111/tW0BAgBo3bqzNmzeXeK7SKHod4+LiHM7kDBs2TN7e3sV+jl1dXTV48OASzX3x4kW5uroWG3dzc7PvB8oLgQgoJ97e3pKks2fPlqj+p59+krOzc7G7dQICAuTr61vm8CJJ9evXd3hcvXp1SdKZM2fKNN9PP/2koKCgYmGv6O2wsvRa9Jzf3ynk5+dn77dIYWGhZs6cqcaNG8vV1VW1atWSn5+f/vOf/ygnJ6dExzt27Jieeuop1ahRQ56envLz81Pnzp0l6Q/n+P7773XPPfeUdGl2v/32m0aPHi1/f3+5u7vLz89PISEhxY45ffp07d+/X/Xq1VPbtm01adIk/fDDD/b9ISEhio+P1zvvvKNatWopMjJSSUlJDnMcOXJEhmGocePG8vPzc9gOHTpkvyC6JHOVRtHr2KRJE4dxm82mBg0aFPvZqFOnjmw2W4nmdnd3V15eXrHxS5cu2fcD5YVriIBy4u3traCgIO3fv79Uz7vRGYY/UlBQcM1xFxeXa44bhlHmY5np73//u1566SXFxMRo6tSpqlGjhpydnRUXF6fCwsI/fH5BQYH+/Oc/67ffftO4ceMUGhoqDw8PnThxQk899VSJ5iiLAQMGaPv27UpISFCrVq3k6empwsJCPfTQQw7HHDBggDp27KgVK1Zo/fr1eu211/Tqq6/qk08+UY8ePSRJM2bM0FNPPaVPP/1U69ev17PPPqvExETt2LFDdevWVWFhoZycnPTFF19c8/X39PS0//mP5qpIpQkxgYGBOnXqVLHxorGgoKBy6wsgEAHl6OGHH9aCBQuUmpqq8PDwG9YGBwersLBQR44ccbjwODMzU9nZ2QoODraPVa9eXdnZ2Q7Pz8/Pv+Yvi5IqTRALDg7Whg0bdPbsWYezRN988419f2kVPefIkSNq0KCBffyXX34pdibro48+UteuXbVw4UKH8ezsbIcLmq+3pn379unbb7/VkiVL9OSTT9rHk5OTS9Rrw4YNSx10z5w5o40bN2ry5MmaMGGCffzIkSPXrA8MDNQzzzyjZ555RllZWWrdurWmTZtmD0SS1KJFC7Vo0UIvvviitm/frg4dOmj+/Pl6+eWX1bBhQxmGoZCQEN19991/2N+N5iqNotfx8OHDDq9jfn6+jh49qoiIiFLNd7VWrVrp3//+twoLCx3ejtu5c6eqVatWonUCJcVbZkA5Gjt2rDw8PDR06FBlZmYW2//9999r1qxZkqSePXtK+u/dO1d7/fXXJUlRUVH2sYYNG2rr1q0OdQsWLLjuGaKS8PDwKBayrqdnz54qKCjQnDlzHMZnzpwpJycnh1/aJRUREaGqVatq9uzZDmeufv/3If33jNfvz24tX75cJ06ccBjz8PCQpGLrKjpjcvUchmHYX4s/0rdvX+3du/ead/5d76zbtY4pFV9fQUFBsberateuraCgIPvbRbm5ubpy5YpDTYsWLeTs7Gyv6dOnj1xcXDR58uRixzQMQ6dPny7xXKUREREhm82mN9980+G4CxcuVE5OjsPPcWn169dPmZmZ+uSTT+xjv/76q5YvX65evXpd8/oioKw4QwSUo4YNG2rp0qUaOHCgmjZt6vBJ1UUfMFf0ydBhYWGKjo7WggULlJ2drc6dO2vXrl1asmSJevfura5du9rnHTp0qIYPH66+ffvqz3/+s/bu3at169Zd93bvkmjTpo3mzZunl19+WY0aNVLt2rXVrVu3a9b26tVLXbt21QsvvKAff/xRYWFhWr9+vT799FPFxcWpYcOGpT6+n5+fnn/+eSUmJurhhx9Wz549tWfPHn3xxRfF1vXwww9rypQpGjx4sO6//37t27dP77//vsMZCem/f/++vr6aP3++vLy85OHhoXbt2ik0NFQNGzbU888/rxMnTsjb21sff/xxia+pSkhI0EcffaT+/fsrJiZGbdq00W+//abPPvtM8+fPV1hYWLHneHt7q1OnTpo+fbouX76sOnXqaP369Tp69KhD3dmzZ1W3bl3169dPYWFh8vT01IYNG7R7927NmDFD0n8//XvkyJHq37+/7r77bl25ckXvvfeeXFxc1LdvX/vaX375ZY0fP14//vijevfuLS8vLx09elQrVqzQ008/reeff75Ec5WGn5+fxo8fr8mTJ+uhhx7SX/7yFx0+fFhz587Vfffdp8cff7zUcxbp16+f2rdvr8GDB+vgwYP2T6ouKCjQ5MmTyzwvcE1m3NoG3Om+/fZbY9iwYcZdd91l2Gw2w8vLy+jQoYMxe/Zshw+Tu3z5sjF58mQjJCTEqFq1qlGvXr1rfjBjQUGBMW7cOPsHLUZGRhrffffddW+73717t8Pziz608OoPSczIyDCioqIMLy+vEn0w49mzZ40xY8YYQUFBRtWqVY3GjRs7fDBjkZLedl+0rsmTJxuBgYE3/GDGS5cuGc8995y9rkOHDkZqaqrRuXPnYn1/+umnRrNmzYwqVao43AZ/8OBBIyIiwvD09DRq1aplDBs2zNi7d+91b9P/vdOnTxsjR4406tSpY/9gx+joaPuHLF7rtvuff/7Z+Otf/2r4+voaPj4+Rv/+/Y2TJ086fIxCXl6ekZCQYISFhRleXl6Gh4eHERYWZsydO9c+zw8//GDExMQYDRs2NNzc3IwaNWoYXbt2NTZs2FCsz48//th44IEHDA8PD8PDw8MIDQ01YmNjjcOHD5d6rt+71m33RebMmWOEhoYaVatWNfz9/Y0RI0Zc94MZS+O3334zhgwZYtSsWdOoVq2a0blz52I/30B5cDKM2/QqSwAAgHLCNUQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDy+GDGEigsLNTJkyfl5eV1U987BQAAbh3DMHT27FkFBQU5fP3LtRCISuDkyZOqV6+e2W0AAIAyOH78+B9+cTGBqASKvszy+PHj8vb2NrkbAABQErm5uapXr57Dl1JfD4GoBIreJvP29iYQAQBwmynJ5S5cVA0AACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACzP1EA0adIkOTk5OWyhoaH2/ZcuXVJsbKxq1qwpT09P9e3bV5mZmQ5zHDt2TFFRUapWrZpq166thIQEXblyxaFmy5Ytat26tVxdXdWoUSMtXrz4ViwPAADcJkw/Q9S8eXOdOnXKvm3bts2+b8yYMVq1apWWL1+ulJQUnTx5Un369LHvLygoUFRUlPLz87V9+3YtWbJEixcv1oQJE+w1R48eVVRUlLp27ar09HTFxcVp6NChWrdu3S1dJwAAqLycDMMwzDr4pEmTtHLlSqWnpxfbl5OTIz8/Py1dulT9+vWTJH3zzTdq2rSpUlNT1b59e33xxRd6+OGHdfLkSfn7+0uS5s+fr3HjxumXX36RzWbTuHHjtGbNGu3fv98+96OPPqrs7GytXbu2RH3m5ubKx8dHOTk5fLkrAAC3idL8/jb9DNGRI0cUFBSkBg0aaNCgQTp27JgkKS0tTZcvX1ZERIS9NjQ0VPXr11dqaqokKTU1VS1atLCHIUmKjIxUbm6uDhw4YK+5eo6imqI5riUvL0+5ubkOGwAAuHNVMfPg7dq10+LFi9WkSROdOnVKkydPVseOHbV//35lZGTIZrPJ19fX4Tn+/v7KyMiQJGVkZDiEoaL9RftuVJObm6uLFy/K3d29WF+JiYmaPHnyH/bfJuEfJV6rGdJee7JEdcemtKjgTm5O/Qn7SlTXYXaHCu6k7L4c9WWJ6lI6da7gTm5O560pJaqb89yqCu6k7EbO6FWiummP96vgTm7OC//8qER1h6ZtquBObk7TF7qVqG7SpEkV28hNKGlvy5a3rdhGbtKA/rtKVBf2UeW95GRvv8gyP9fUQNSjRw/7n1u2bKl27dopODhYy5Ytu2ZQuVXGjx+v+Ph4++Pc3FzVq1fPtH4AAEDFMv0ts6v5+vrq7rvv1nfffaeAgADl5+crOzvboSYzM1MBAQGSpICAgGJ3nRU9/qMab2/v64YuV1dXeXt7O2wAAODOVakC0blz5/T9998rMDBQbdq0UdWqVbVx40b7/sOHD+vYsWMKDw+XJIWHh2vfvn3Kysqy1yQnJ8vb21vNmjWz11w9R1FN0RwAAACmBqLnn39eKSkp+vHHH7V9+3b99a9/lYuLix577DH5+PhoyJAhio+P1+bNm5WWlqbBgwcrPDxc7du3lyR1795dzZo10xNPPKG9e/dq3bp1evHFFxUbGytXV1dJ0vDhw/XDDz9o7Nix+uabbzR37lwtW7ZMY8aMMXPpAACgEjH1GqKff/5Zjz32mE6fPi0/Pz898MAD2rFjh/z8/CRJM2fOlLOzs/r27au8vDxFRkZq7ty59ue7uLho9erVGjFihMLDw+Xh4aHo6GhNmTLFXhMSEqI1a9ZozJgxmjVrlurWrat33nlHkZFlv/AKAADcWUwNRB988MEN97u5uSkpKUlJSUnXrQkODtbnn39+w3m6dOmiPXv2lKlHAABw56tU1xABAACYgUAEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsr9IEoldeeUVOTk6Ki4uzj126dEmxsbGqWbOmPD091bdvX2VmZjo879ixY4qKilK1atVUu3ZtJSQk6MqVKw41W7ZsUevWreXq6qpGjRpp8eLFt2BFAADgdlEpAtHu3bv11ltvqWXLlg7jY8aM0apVq7R8+XKlpKTo5MmT6tOnj31/QUGBoqKilJ+fr+3bt2vJkiVavHixJkyYYK85evSooqKi1LVrV6WnpysuLk5Dhw7VunXrbtn6AABA5WZ6IDp37pwGDRqkt99+W9WrV7eP5+TkaOHChXr99dfVrVs3tWnTRosWLdL27du1Y8cOSdL69et18OBB/fOf/1SrVq3Uo0cPTZ06VUlJScrPz5ckzZ8/XyEhIZoxY4aaNm2qkSNHql+/fpo5c6Yp6wUAAJWP6YEoNjZWUVFRioiIcBhPS0vT5cuXHcZDQ0NVv359paamSpJSU1PVokUL+fv722siIyOVm5urAwcO2Gt+P3dkZKR9jmvJy8tTbm6uwwYAAO5cVcw8+AcffKCvv/5au3fvLrYvIyNDNptNvr6+DuP+/v7KyMiw11wdhor2F+27UU1ubq4uXrwod3f3YsdOTEzU5MmTy7wuAABwezHtDNHx48c1evRovf/++3JzczOrjWsaP368cnJy7Nvx48fNbgkAAFQg0wJRWlqasrKy1Lp1a1WpUkVVqlRRSkqK3nzzTVWpUkX+/v7Kz89Xdna2w/MyMzMVEBAgSQoICCh211nR4z+q8fb2vubZIUlydXWVt7e3wwYAAO5cpgWiBx98UPv27VN6erp9u/feezVo0CD7n6tWraqNGzfan3P48GEdO3ZM4eHhkqTw8HDt27dPWVlZ9prk5GR5e3urWbNm9pqr5yiqKZoDAADAtGuIvLy8dM899ziMeXh4qGbNmvbxIUOGKD4+XjVq1JC3t7dGjRql8PBwtW/fXpLUvXt3NWvWTE888YSmT5+ujIwMvfjii4qNjZWrq6skafjw4ZozZ47Gjh2rmJgYbdq0ScuWLdOaNWtu7YIBAEClZepF1X9k5syZcnZ2Vt++fZWXl6fIyEjNnTvXvt/FxUWrV6/WiBEjFB4eLg8PD0VHR2vKlCn2mpCQEK1Zs0ZjxozRrFmzVLduXb3zzjuKjIw0Y0kAAKASqlSBaMuWLQ6P3dzclJSUpKSkpOs+Jzg4WJ9//vkN5+3SpYv27NlTHi0CAIA7kOmfQwQAAGA2AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8UwPRvHnz1LJlS3l7e8vb21vh4eH64osv7PsvXbqk2NhY1axZU56enurbt68yMzMd5jh27JiioqJUrVo11a5dWwkJCbpy5YpDzZYtW9S6dWu5urqqUaNGWrx48a1YHgAAuE2YGojq1q2rV155RWlpafrqq6/UrVs3PfLIIzpw4IAkacyYMVq1apWWL1+ulJQUnTx5Un369LE/v6CgQFFRUcrPz9f27du1ZMkSLV68WBMmTLDXHD16VFFRUeratavS09MVFxenoUOHat26dbd8vQAAoHKqYubBe/Xq5fB42rRpmjdvnnbs2KG6detq4cKFWrp0qbp16yZJWrRokZo2baodO3aoffv2Wr9+vQ4ePKgNGzbI399frVq10tSpUzVu3DhNmjRJNptN8+fPV0hIiGbMmCFJatq0qbZt26aZM2cqMjLylq8ZAABUPpXmGqKCggJ98MEHOn/+vMLDw5WWlqbLly8rIiLCXhMaGqr69esrNTVVkpSamqoWLVrI39/fXhMZGanc3Fz7WabU1FSHOYpqiua4lry8POXm5jpsAADgzmV6INq3b588PT3l6uqq4cOHa8WKFWrWrJkyMjJks9nk6+vrUO/v76+MjAxJUkZGhkMYKtpftO9GNbm5ubp48eI1e0pMTJSPj499q1evXnksFQAAVFKmB6ImTZooPT1dO3fu1IgRIxQdHa2DBw+a2tP48eOVk5Nj344fP25qPwAAoGKZeg2RJNlsNjVq1EiS1KZNG+3evVuzZs3SwIEDlZ+fr+zsbIezRJmZmQoICJAkBQQEaNeuXQ7zFd2FdnXN7+9My8zMlLe3t9zd3a/Zk6urq1xdXctlfQAAoPIz/QzR7xUWFiovL09t2rRR1apVtXHjRvu+w4cP69ixYwoPD5ckhYeHa9++fcrKyrLXJCcny9vbW82aNbPXXD1HUU3RHAAAAKaeIRo/frx69Oih+vXr6+zZs1q6dKm2bNmidevWycfHR0OGDFF8fLxq1Kghb29vjRo1SuHh4Wrfvr0kqXv37mrWrJmeeOIJTZ8+XRkZGXrxxRcVGxtrP8MzfPhwzZkzR2PHjlVMTIw2bdqkZcuWac2aNWYuHQAAVCKmBqKsrCw9+eSTOnXqlHx8fNSyZUutW7dOf/7znyVJM2fOlLOzs/r27au8vDxFRkZq7ty59ue7uLho9erVGjFihMLDw+Xh4aHo6GhNmTLFXhMSEqI1a9ZozJgxmjVrlurWrat33nmHW+4BAICdqYFo4cKFN9zv5uampKQkJSUlXbcmODhYn3/++Q3n6dKli/bs2VOmHgEAwJ2v0l1DBAAAcKsRiAAAgOURiAAAgOWVKRB169ZN2dnZxcZzc3Pt3zsGAABwuyhTINqyZYvy8/OLjV+6dEn//ve/b7opAACAW6lUd5n95z//sf/54MGD9u8Lk/775axr165VnTp1yq87AACAW6BUgahVq1ZycnKSk5PTNd8ac3d31+zZs8utOQAAgFuhVIHo6NGjMgxDDRo00K5du+Tn52ffZ7PZVLt2bbm4uJR7kwAAABWpVIEoODhY0n+/bwwAAOBOUeZPqj5y5Ig2b96srKysYgFpwoQJN90YAADArVKmQPT2229rxIgRqlWrlgICAuTk5GTf5+TkRCACAAC3lTIFopdfflnTpk3TuHHjyrsfAACAW65Mn0N05swZ9e/fv7x7AQAAMEWZAlH//v21fv368u4FAADAFGV6y6xRo0Z66aWXtGPHDrVo0UJVq1Z12P/ss8+WS3MAAAC3QpkC0YIFC+Tp6amUlBSlpKQ47HNyciIQAQCA20qZAtHRo0fLuw8AAADTlOkaIgAAgDtJmc4QxcTE3HD/u+++W6ZmAAAAzFCmQHTmzBmHx5cvX9b+/fuVnZ19zS99BQAAqMzKFIhWrFhRbKywsFAjRoxQw4YNb7opAACAW6ncriFydnZWfHy8Zs6cWV5TAgAA3BLlelH1999/rytXrpTnlAAAABWuTG+ZxcfHOzw2DEOnTp3SmjVrFB0dXS6NAQAA3CplCkR79uxxeOzs7Cw/Pz/NmDHjD+9AAwAAqGzKFIg2b95c3n0AAACYpkyBqMgvv/yiw4cPS5KaNGkiPz+/cmkKAADgVirTRdXnz59XTEyMAgMD1alTJ3Xq1ElBQUEaMmSILly4UN49AgAAVKgyBaL4+HilpKRo1apVys7OVnZ2tj799FOlpKToueeeK+8eAQAAKlSZ3jL7+OOP9dFHH6lLly72sZ49e8rd3V0DBgzQvHnzyqs/AACAClemM0QXLlyQv79/sfHatWvzlhkAALjtlCkQhYeHa+LEibp06ZJ97OLFi5o8ebLCw8PLrTkAAIBboUxvmb3xxht66KGHVLduXYWFhUmS9u7dK1dXV61fv75cGwQAAKhoZQpELVq00JEjR/T+++/rm2++kSQ99thjGjRokNzd3cu1QQAAgIpWpkCUmJgof39/DRs2zGH83Xff1S+//KJx48aVS3MAAAC3QpmuIXrrrbcUGhpabLx58+aaP3/+TTcFAABwK5UpEGVkZCgwMLDYuJ+fn06dOnXTTQEAANxKZQpE9erV05dfflls/Msvv1RQUNBNNwUAAHArlekaomHDhikuLk6XL19Wt27dJEkbN27U2LFj+aRqAABw2ylTIEpISNDp06f1zDPPKD8/X5Lk5uamcePGafz48eXaIAAAQEUrUyBycnLSq6++qpdeekmHDh2Su7u7GjduLFdX1/LuDwAAoMKVKRAV8fT01H333VdevQAAAJiiTBdVAwAA3EkIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPJMDUSJiYm677775OXlpdq1a6t37946fPiwQ82lS5cUGxurmjVrytPTU3379lVmZqZDzbFjxxQVFaVq1aqpdu3aSkhI0JUrVxxqtmzZotatW8vV1VWNGjXS4sWLK3p5AADgNmFqIEpJSVFsbKx27Nih5ORkXb58Wd27d9f58+ftNWPGjNGqVau0fPlypaSk6OTJk+rTp499f0FBgaKiopSfn6/t27dryZIlWrx4sSZMmGCvOXr0qKKiotS1a1elp6crLi5OQ4cO1bp1627pegEAQOVUxcyDr1271uHx4sWLVbt2baWlpalTp07KycnRwoULtXTpUnXr1k2StGjRIjVt2lQ7duxQ+/bttX79eh08eFAbNmyQv7+/WrVqpalTp2rcuHGaNGmSbDab5s+fr5CQEM2YMUOS1LRpU23btk0zZ85UZGRksb7y8vKUl5dnf5ybm1uBfwsAAMBsleoaopycHElSjRo1JElpaWm6fPmyIiIi7DWhoaGqX7++UlNTJUmpqalq0aKF/P397TWRkZHKzc3VgQMH7DVXz1FUUzTH7yUmJsrHx8e+1atXr/wWCQAAKp1KE4gKCwsVFxenDh066J577pEkZWRkyGazydfX16HW399fGRkZ9pqrw1DR/qJ9N6rJzc3VxYsXi/Uyfvx45eTk2Lfjx4+XyxoBAEDlZOpbZleLjY3V/v37tW3bNrNbkaurq1xdXc1uAwAA3CKV4gzRyJEjtXr1am3evFl169a1jwcEBCg/P1/Z2dkO9ZmZmQoICLDX/P6us6LHf1Tj7e0td3f38l4OAAC4zZgaiAzD0MiRI7VixQpt2rRJISEhDvvbtGmjqlWrauPGjfaxw4cP69ixYwoPD5ckhYeHa9++fcrKyrLXJCcny9vbW82aNbPXXD1HUU3RHAAAwNpMfcssNjZWS5cu1aeffiovLy/7NT8+Pj5yd3eXj4+PhgwZovj4eNWoUUPe3t4aNWqUwsPD1b59e0lS9+7d1axZMz3xxBOaPn26MjIy9OKLLyo2Ntb+ttfw4cM1Z84cjR07VjExMdq0aZOWLVumNWvWmLZ2AABQeZh6hmjevHnKyclRly5dFBgYaN8+/PBDe83MmTP18MMPq2/fvurUqZMCAgL0ySef2Pe7uLho9erVcnFxUXh4uB5//HE9+eSTmjJlir0mJCREa9asUXJyssLCwjRjxgy9884717zlHgAAWI+pZ4gMw/jDGjc3NyUlJSkpKem6NcHBwfr8889vOE+XLl20Z8+eUvcIAADufJXiomoAAAAzEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlmRqItm7dql69eikoKEhOTk5auXKlw37DMDRhwgQFBgbK3d1dEREROnLkiEPNb7/9pkGDBsnb21u+vr4aMmSIzp0751Dzn//8Rx07dpSbm5vq1aun6dOnV/TSAADAbcTUQHT+/HmFhYUpKSnpmvunT5+uN998U/Pnz9fOnTvl4eGhyMhIXbp0yV4zaNAgHThwQMnJyVq9erW2bt2qp59+2r4/NzdX3bt3V3BwsNLS0vTaa69p0qRJWrBgQYWvDwAA3B6qmHnwHj16qEePHtfcZxiG3njjDb344ot65JFHJEn/+Mc/5O/vr5UrV+rRRx/VoUOHtHbtWu3evVv33nuvJGn27Nnq2bOn/u///k9BQUF6//33lZ+fr3fffVc2m03NmzdXenq6Xn/9dYfgBAAArKvSXkN09OhRZWRkKCIiwj7m4+Ojdu3aKTU1VZKUmpoqX19fexiSpIiICDk7O2vnzp32mk6dOslms9lrIiMjdfjwYZ05c+aax87Ly1Nubq7DBgAA7lyVNhBlZGRIkvz9/R3G/f397fsyMjJUu3Zth/1VqlRRjRo1HGquNcfVx/i9xMRE+fj42Ld69erd/IIAAEClVWkDkZnGjx+vnJwc+3b8+HGzWwIAABWo0gaigIAASVJmZqbDeGZmpn1fQECAsrKyHPZfuXJFv/32m0PNtea4+hi/5+rqKm9vb4cNAADcuSptIAoJCVFAQIA2btxoH8vNzdXOnTsVHh4uSQoPD1d2drbS0tLsNZs2bVJhYaHatWtnr9m6dasuX75sr0lOTlaTJk1UvXr1W7QaAABQmZkaiM6dO6f09HSlp6dL+u+F1Onp6Tp27JicnJwUFxenl19+WZ999pn27dunJ598UkFBQerdu7ckqWnTpnrooYc0bNgw7dq1S19++aVGjhypRx99VEFBQZKk//mf/5HNZtOQIUN04MABffjhh5o1a5bi4+NNWjUAAKhsTL3t/quvvlLXrl3tj4tCSnR0tBYvXqyxY8fq/Pnzevrpp5Wdna0HHnhAa9eulZubm/0577//vkaOHKkHH3xQzs7O6tu3r9588037fh8fH61fv16xsbFq06aNatWqpQkTJnDLPQAAsDM1EHXp0kWGYVx3v5OTk6ZMmaIpU6Zct6ZGjRpaunTpDY/TsmVL/fvf/y5znwAA4M5Waa8hAgAAuFUIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIsFYiSkpJ01113yc3NTe3atdOuXbvMbgkAAFQClglEH374oeLj4zVx4kR9/fXXCgsLU2RkpLKyssxuDQAAmMwygej111/XsGHDNHjwYDVr1kzz589XtWrV9O6775rdGgAAMFkVsxu4FfLz85WWlqbx48fbx5ydnRUREaHU1NRi9Xl5ecrLy7M/zsnJkSTl5uY61BXkXaygjsvH7/u9nrOXCiq4k5tT0nVcuXilgjspu5Ku4fyVyrsGqeTruJh3oYI7KbuSruHS5csV3MnNKek6zl06X8Gd3JySruPq/ydXNiVdw4ULd8b/awsuVN6fqd+voeixYRh//GTDAk6cOGFIMrZv3+4wnpCQYLRt27ZY/cSJEw1JbGxsbGxsbHfAdvz48T/MCpY4Q1Ra48ePV3x8vP1xYWGhfvvtN9WsWVNOTk4Vcszc3FzVq1dPx48fl7e3d4Uc41a4E9ZxJ6xBYh2VyZ2wBunOWMedsAaJdZSUYRg6e/asgoKC/rDWEoGoVq1acnFxUWZmpsN4ZmamAgICitW7urrK1dXVYczX17ciW7Tz9va+rX+4i9wJ67gT1iCxjsrkTliDdGes405Yg8Q6SsLHx6dEdZa4qNpms6lNmzbauHGjfaywsFAbN25UeHi4iZ0BAIDKwBJniCQpPj5e0dHRuvfee9W2bVu98cYbOn/+vAYPHmx2awAAwGSWCUQDBw7UL7/8ogkTJigjI0OtWrXS2rVr5e/vb3Zrkv77Nt3EiROLvVV3u7kT1nEnrEFiHZXJnbAG6c5Yx52wBol1VAQnwyjJvWgAAAB3LktcQwQAAHAjBCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BKJKIikpSXfddZfc3NzUrl077dq1y+yWSmXr1q3q1auXgoKC5OTkpJUrV5rdUqklJibqvvvuk5eXl2rXrq3evXvr8OHDZrdVavPmzVPLli3tn/waHh6uL774wuy2bsorr7wiJycnxcXFmd1KqUyaNElOTk4OW2hoqNltldqJEyf0+OOPq2bNmnJ3d1eLFi301Vdfmd1Wqdx1113FXgsnJyfFxsaa3VqpFBQU6KWXXlJISIjc3d3VsGFDTZ06tWRfXlqJnD17VnFxcQoODpa7u7vuv/9+7d6929SeCESVwIcffqj4+HhNnDhRX3/9tcLCwhQZGamsrCyzWyux8+fPKywsTElJSWa3UmYpKSmKjY3Vjh07lJycrMuXL6t79+46f77yfrPztdStW1evvPKK0tLS9NVXX6lbt2565JFHdODAAbNbK5Pdu3frrbfeUsuWLc1upUyaN2+uU6dO2bdt27aZ3VKpnDlzRh06dFDVqlX1xRdf6ODBg5oxY4aqV69udmulsnv3bofXITk5WZLUv39/kzsrnVdffVXz5s3TnDlzdOjQIb366quaPn26Zs+ebXZrpTJ06FAlJyfrvffe0759+9S9e3dFREToxIkT5jVVLl8nj5vStm1bIzY21v64oKDACAoKMhITE03squwkGStWrDC7jZuWlZVlSDJSUlLMbuWmVa9e3XjnnXfMbqPUzp49azRu3NhITk42OnfubIwePdrslkpl4sSJRlhYmNlt3JRx48YZDzzwgNltlLvRo0cbDRs2NAoLC81upVSioqKMmJgYh7E+ffoYgwYNMqmj0rtw4YLh4uJirF692mG8devWxgsvvGBSV4bBGSKT5efnKy0tTREREfYxZ2dnRUREKDU11cTOkJOTI0mqUaOGyZ2UXUFBgT744AOdP3/+tvzevtjYWEVFRTn893G7OXLkiIKCgtSgQQMNGjRIx44dM7ulUvnss8907733qn///qpdu7b+9Kc/6e233za7rZuSn5+vf/7zn4qJiZGTk5PZ7ZTK/fffr40bN+rbb7+VJO3du1fbtm1Tjx49TO6s5K5cuaKCggK5ubk5jLu7u5t6BtUyX91RWf36668qKCgo9hUi/v7++uabb0zqCoWFhYqLi1OHDh10zz33mN1Oqe3bt0/h4eG6dOmSPD09tWLFCjVr1szstkrlgw8+0Ndff236dQU3o127dlq8eLGaNGmiU6dOafLkyerYsaP2798vLy8vs9srkR9++EHz5s1TfHy8/vd//1e7d+/Ws88+K5vNpujoaLPbK5OVK1cqOztbTz31lNmtlNrf/vY35ebmKjQ0VC4uLiooKNC0adM0aNAgs1srMS8vL4WHh2vq1Klq2rSp/P399a9//Uupqalq1KiRaX0RiIBriI2N1f79+2+76z2KNGnSROnp6crJydFHH32k6OhopaSk3Dah6Pjx4xo9erSSk5OL/SvydnL1v9pbtmypdu3aKTg4WMuWLdOQIUNM7KzkCgsLde+99+rvf/+7JOlPf/qT9u/fr/nz59+2gWjhwoXq0aOHgoKCzG6l1JYtW6b3339fS5cuVfPmzZWenq64uDgFBQXdVq/He++9p5iYGNWpU0cuLi5q3bq1HnvsMaWlpZnWE4HIZLVq1ZKLi4syMzMdxjMzMxUQEGBSV9Y2cuRIrV69Wlu3blXdunXNbqdMbDab/V9abdq00e7duzVr1iy99dZbJndWMmlpacrKylLr1q3tYwUFBdq6davmzJmjvLw8ubi4mNhh2fj6+uruu+/Wd999Z3YrJRYYGFgsSDdt2lQff/yxSR3dnJ9++kkbNmzQJ598YnYrZZKQkKC//e1vevTRRyVJLVq00E8//aTExMTbKhA1bNhQKSkpOn/+vHJzcxUYGKiBAweqQYMGpvXENUQms9lsatOmjTZu3GgfKyws1MaNG2/Laz5uZ4ZhaOTIkVqxYoU2bdqkkJAQs1sqN4WFhcrLyzO7jRJ78MEHtW/fPqWnp9u3e++9V4MGDVJ6evptGYYk6dy5c/r+++8VGBhodisl1qFDh2IfP/Htt98qODjYpI5uzqJFi1S7dm1FRUWZ3UqZXLhwQc7Ojr+6XVxcVFhYaFJHN8fDw0OBgYE6c+aM1q1bp0ceecS0XjhDVAnEx8crOjpa9957r9q2bas33nhD58+f1+DBg81urcTOnTvn8K/eo0ePKj09XTVq1FD9+vVN7KzkYmNjtXTpUn366afy8vJSRkaGJMnHx0fu7u4md1dy48ePV48ePVS/fn2dPXtWS5cu1ZYtW7Ru3TqzWysxLy+vYtdueXh4qGbNmrfVNV3PP/+8evXqpeDgYJ08eVITJ06Ui4uLHnvsMbNbK7ExY8bo/vvv19///ncNGDBAu3bt0oIFC7RgwQKzWyu1wsJCLVq0SNHR0apS5fb89derVy9NmzZN9evXV/PmzbVnzx69/vrriomJMbu1Ulm3bp0Mw1CTJk303XffKSEhQaGhoeb+3jPt/jY4mD17tlG/fn3DZrMZbdu2NXbs2GF2S6WyefNmQ1KxLTo62uzWSuxa/UsyFi1aZHZrpRITE2MEBwcbNpvN8PPzMx588EFj/fr1Zrd1027H2+4HDhxoBAYGGjabzahTp44xcOBA47vvvjO7rVJbtWqVcc899xiurq5GaGiosWDBArNbKpN169YZkozDhw+b3UqZ5ebmGqNHjzbq169vuLm5GQ0aNDBeeOEFIy8vz+zWSuXDDz80GjRoYNhsNiMgIMCIjY01srOzTe3JyTBus4+3BAAAKGdcQwQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACzv/wOgoqMAnWSYrQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range (len(x_clinet_list)):\n",
    "    print(len(y_client_list[i]))\n",
    "    getDist(y_client_list[i],num_classes,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "dev = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG = Generator().to(dev)\n",
    "# netD = Discriminator().to(dev)\n",
    "# summary(netG,(128,1,1))\n",
    "# summary(netD,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_server = Server(0,LEARNING_RATE)\n",
    "main_server.generator.train()\n",
    "workers = []\n",
    "for i in range(num_workers):\n",
    "    worker = Worker(i,LEARNING_RATE)\n",
    "    # x_clinet_list[i] = np.transpose(x_clinet_list[i],(0, 3, 1, 2))\n",
    "    worker.load_worker_data(x_clinet_list[i], y_client_list[i])\n",
    "    worker.discriminator.train()\n",
    "    workers.append(worker)\n",
    "    \n",
    "# summary(main_server.generator,(128,1,1))\n",
    "# summary(workers[0].discriminator,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(dev)\n",
    "\n",
    "worker_loaders = []\n",
    "\n",
    "for worker in workers:\n",
    "    # print(worker.x_data.shape)\n",
    "    worker_loaders.append([])\n",
    "    for batch_id, real in enumerate(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)):\n",
    "        worker_loaders[-1].append(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/782                  Loss D: 0.5007, loss G: 0.7759\n",
      "Epoch [0/50] Batch 100/782                  Loss D: 0.0379, loss G: 1.1310\n",
      "Epoch [0/50] Batch 200/782                  Loss D: 0.1520, loss G: 0.9542\n",
      "Epoch [0/50] Batch 300/782                  Loss D: 0.0328, loss G: 1.0187\n",
      "Epoch [0/50] Batch 400/782                  Loss D: 0.0272, loss G: 0.9776\n",
      "Epoch [0/50] Batch 500/782                  Loss D: 0.3674, loss G: 1.0421\n",
      "Epoch [0/50] Batch 600/782                  Loss D: 0.0201, loss G: 0.9809\n",
      "Epoch [0/50] Batch 700/782                  Loss D: 0.0516, loss G: 1.0096\n",
      "Epoch [1/50] Batch 0/782                  Loss D: 0.0920, loss G: 0.7543\n",
      "Epoch [1/50] Batch 100/782                  Loss D: 0.0884, loss G: 0.8475\n",
      "Epoch [1/50] Batch 200/782                  Loss D: 0.0729, loss G: 1.0511\n",
      "Epoch [1/50] Batch 300/782                  Loss D: 0.0595, loss G: 0.9736\n",
      "Epoch [1/50] Batch 400/782                  Loss D: 0.0599, loss G: 0.9672\n",
      "Epoch [1/50] Batch 500/782                  Loss D: 0.0716, loss G: 0.6898\n",
      "Epoch [1/50] Batch 600/782                  Loss D: 0.0960, loss G: 0.8877\n",
      "Epoch [1/50] Batch 700/782                  Loss D: 0.0574, loss G: 0.8521\n",
      "Epoch [2/50] Batch 0/782                  Loss D: 0.0578, loss G: 0.8006\n",
      "Epoch [2/50] Batch 100/782                  Loss D: 0.0665, loss G: 0.7165\n",
      "Epoch [2/50] Batch 200/782                  Loss D: 0.0641, loss G: 0.8662\n",
      "Epoch [2/50] Batch 300/782                  Loss D: 0.1633, loss G: 1.0006\n",
      "Epoch [2/50] Batch 400/782                  Loss D: 0.0890, loss G: 0.9327\n",
      "Epoch [2/50] Batch 500/782                  Loss D: 0.0666, loss G: 0.9378\n",
      "Epoch [2/50] Batch 600/782                  Loss D: 0.1042, loss G: 0.7167\n",
      "Epoch [2/50] Batch 700/782                  Loss D: 0.0486, loss G: 0.8728\n",
      "Epoch [3/50] Batch 0/782                  Loss D: 0.0832, loss G: 0.8616\n",
      "Epoch [3/50] Batch 100/782                  Loss D: 0.0785, loss G: 1.0068\n",
      "Epoch [3/50] Batch 200/782                  Loss D: 0.0919, loss G: 0.9461\n",
      "Epoch [3/50] Batch 300/782                  Loss D: 0.0895, loss G: 0.9984\n",
      "Epoch [3/50] Batch 400/782                  Loss D: 0.1135, loss G: 0.9362\n",
      "Epoch [3/50] Batch 500/782                  Loss D: 0.0562, loss G: 1.1332\n",
      "Epoch [3/50] Batch 600/782                  Loss D: 0.0561, loss G: 1.0206\n",
      "Epoch [3/50] Batch 700/782                  Loss D: 0.0611, loss G: 1.0081\n",
      "Epoch [4/50] Batch 0/782                  Loss D: 0.0968, loss G: 1.1112\n",
      "Epoch [4/50] Batch 100/782                  Loss D: 0.1119, loss G: 0.7336\n",
      "Epoch [4/50] Batch 200/782                  Loss D: 0.0714, loss G: 0.8815\n",
      "Epoch [4/50] Batch 300/782                  Loss D: 0.0551, loss G: 0.7678\n",
      "Epoch [4/50] Batch 400/782                  Loss D: 0.1264, loss G: 0.8683\n",
      "Epoch [4/50] Batch 500/782                  Loss D: 0.0711, loss G: 0.7853\n",
      "Epoch [4/50] Batch 600/782                  Loss D: 0.0577, loss G: 0.8935\n",
      "Epoch [4/50] Batch 700/782                  Loss D: 0.0415, loss G: 0.8732\n",
      "Epoch [5/50] Batch 0/782                  Loss D: 0.0942, loss G: 0.7697\n",
      "Epoch [5/50] Batch 100/782                  Loss D: 0.0416, loss G: 1.0648\n",
      "Epoch [5/50] Batch 200/782                  Loss D: 0.0818, loss G: 0.7717\n",
      "Epoch [5/50] Batch 300/782                  Loss D: 0.0431, loss G: 0.9275\n",
      "Epoch [5/50] Batch 400/782                  Loss D: 0.1053, loss G: 0.7828\n",
      "Epoch [5/50] Batch 500/782                  Loss D: 0.0533, loss G: 0.9562\n",
      "Epoch [5/50] Batch 600/782                  Loss D: 0.0953, loss G: 1.0750\n",
      "Epoch [5/50] Batch 700/782                  Loss D: 0.0499, loss G: 0.8703\n",
      "Epoch [6/50] Batch 0/782                  Loss D: 0.0593, loss G: 0.8271\n",
      "Epoch [6/50] Batch 100/782                  Loss D: 0.0521, loss G: 0.9169\n",
      "Epoch [6/50] Batch 200/782                  Loss D: 0.1181, loss G: 1.0537\n",
      "Epoch [6/50] Batch 300/782                  Loss D: 0.0880, loss G: 0.8718\n",
      "Epoch [6/50] Batch 400/782                  Loss D: 0.0391, loss G: 1.1244\n",
      "Epoch [6/50] Batch 500/782                  Loss D: 0.0724, loss G: 0.8937\n",
      "Epoch [6/50] Batch 600/782                  Loss D: 0.0595, loss G: 1.1448\n",
      "Epoch [6/50] Batch 700/782                  Loss D: 0.0948, loss G: 0.5790\n",
      "Epoch [7/50] Batch 0/782                  Loss D: 0.0531, loss G: 0.9606\n",
      "Epoch [7/50] Batch 100/782                  Loss D: 0.0789, loss G: 0.8780\n",
      "Epoch [7/50] Batch 200/782                  Loss D: 0.1288, loss G: 1.0769\n",
      "Epoch [7/50] Batch 300/782                  Loss D: 0.0386, loss G: 0.9539\n",
      "Epoch [7/50] Batch 400/782                  Loss D: 0.0734, loss G: 0.7382\n",
      "Epoch [7/50] Batch 500/782                  Loss D: 0.0832, loss G: 1.0303\n",
      "Epoch [7/50] Batch 600/782                  Loss D: 0.1509, loss G: 0.4500\n",
      "Epoch [7/50] Batch 700/782                  Loss D: 0.0545, loss G: 0.8585\n",
      "Epoch [8/50] Batch 0/782                  Loss D: 0.0958, loss G: 0.9835\n",
      "Epoch [8/50] Batch 100/782                  Loss D: 0.0662, loss G: 0.8436\n",
      "Epoch [8/50] Batch 200/782                  Loss D: 0.0766, loss G: 0.9678\n",
      "Epoch [8/50] Batch 300/782                  Loss D: 0.0380, loss G: 1.1533\n",
      "Epoch [8/50] Batch 400/782                  Loss D: 0.0739, loss G: 0.8027\n",
      "Epoch [8/50] Batch 500/782                  Loss D: 0.0446, loss G: 0.8659\n",
      "Epoch [8/50] Batch 600/782                  Loss D: 0.0612, loss G: 1.0367\n",
      "Epoch [8/50] Batch 700/782                  Loss D: 0.0925, loss G: 0.8195\n",
      "Epoch [9/50] Batch 0/782                  Loss D: 0.0801, loss G: 0.9440\n",
      "Epoch [9/50] Batch 100/782                  Loss D: 0.0541, loss G: 0.9251\n",
      "Epoch [9/50] Batch 200/782                  Loss D: 0.0787, loss G: 0.8624\n",
      "Epoch [9/50] Batch 300/782                  Loss D: 0.1568, loss G: 0.6752\n",
      "Epoch [9/50] Batch 400/782                  Loss D: 0.0847, loss G: 0.9653\n",
      "Epoch [9/50] Batch 500/782                  Loss D: 0.0769, loss G: 1.0977\n",
      "Epoch [9/50] Batch 600/782                  Loss D: 0.0962, loss G: 0.8582\n",
      "Epoch [9/50] Batch 700/782                  Loss D: 0.0647, loss G: 0.9845\n",
      "Epoch [10/50] Batch 0/782                  Loss D: 0.0757, loss G: 0.7956\n",
      "Epoch [10/50] Batch 100/782                  Loss D: 0.0887, loss G: 0.8358\n",
      "Epoch [10/50] Batch 200/782                  Loss D: 0.0857, loss G: 0.7944\n",
      "Epoch [10/50] Batch 300/782                  Loss D: 0.0487, loss G: 0.8159\n",
      "Epoch [10/50] Batch 400/782                  Loss D: 0.0806, loss G: 0.6280\n",
      "Epoch [10/50] Batch 500/782                  Loss D: 0.0511, loss G: 0.8591\n",
      "Epoch [10/50] Batch 600/782                  Loss D: 0.0820, loss G: 0.9763\n",
      "Epoch [10/50] Batch 700/782                  Loss D: 0.0707, loss G: 1.0364\n",
      "Epoch [11/50] Batch 0/782                  Loss D: 0.0933, loss G: 1.1028\n",
      "Epoch [11/50] Batch 100/782                  Loss D: 0.0476, loss G: 0.9002\n",
      "Epoch [11/50] Batch 200/782                  Loss D: 0.1013, loss G: 0.8158\n",
      "Epoch [11/50] Batch 300/782                  Loss D: 0.0677, loss G: 0.9122\n",
      "Epoch [11/50] Batch 400/782                  Loss D: 0.0501, loss G: 0.8873\n",
      "Epoch [11/50] Batch 500/782                  Loss D: 0.0576, loss G: 0.9540\n",
      "Epoch [11/50] Batch 600/782                  Loss D: 0.0793, loss G: 0.8545\n",
      "Epoch [11/50] Batch 700/782                  Loss D: 0.0526, loss G: 0.8223\n",
      "Epoch [12/50] Batch 0/782                  Loss D: 0.1596, loss G: 0.5830\n",
      "Epoch [12/50] Batch 100/782                  Loss D: 0.1269, loss G: 0.8710\n",
      "Epoch [12/50] Batch 200/782                  Loss D: 0.0985, loss G: 0.8331\n",
      "Epoch [12/50] Batch 300/782                  Loss D: 0.1150, loss G: 0.7994\n",
      "Epoch [12/50] Batch 400/782                  Loss D: 0.1028, loss G: 0.8516\n",
      "Epoch [12/50] Batch 500/782                  Loss D: 0.0906, loss G: 0.8397\n",
      "Epoch [12/50] Batch 600/782                  Loss D: 0.0972, loss G: 1.1061\n",
      "Epoch [12/50] Batch 700/782                  Loss D: 0.0784, loss G: 0.9962\n",
      "Epoch [13/50] Batch 0/782                  Loss D: 0.1059, loss G: 0.8597\n",
      "Epoch [13/50] Batch 100/782                  Loss D: 0.0725, loss G: 0.8961\n",
      "Epoch [13/50] Batch 200/782                  Loss D: 0.0508, loss G: 0.9668\n",
      "Epoch [13/50] Batch 300/782                  Loss D: 0.1071, loss G: 0.9557\n",
      "Epoch [13/50] Batch 400/782                  Loss D: 0.0938, loss G: 0.9759\n",
      "Epoch [13/50] Batch 500/782                  Loss D: 0.0471, loss G: 1.0471\n",
      "Epoch [13/50] Batch 600/782                  Loss D: 0.0939, loss G: 0.7119\n",
      "Epoch [13/50] Batch 700/782                  Loss D: 0.0756, loss G: 1.0428\n",
      "Epoch [14/50] Batch 0/782                  Loss D: 0.0686, loss G: 0.9786\n",
      "Epoch [14/50] Batch 100/782                  Loss D: 0.0596, loss G: 1.0947\n",
      "Epoch [14/50] Batch 200/782                  Loss D: 0.0888, loss G: 0.6523\n",
      "Epoch [14/50] Batch 300/782                  Loss D: 0.0503, loss G: 0.9409\n",
      "Epoch [14/50] Batch 400/782                  Loss D: 0.0522, loss G: 0.7517\n",
      "Epoch [14/50] Batch 500/782                  Loss D: 0.0862, loss G: 0.9879\n",
      "Epoch [14/50] Batch 600/782                  Loss D: 0.0730, loss G: 0.7009\n",
      "Epoch [14/50] Batch 700/782                  Loss D: 0.0682, loss G: 0.8188\n",
      "Epoch [15/50] Batch 0/782                  Loss D: 0.0563, loss G: 0.6789\n",
      "Epoch [15/50] Batch 100/782                  Loss D: 0.0909, loss G: 0.7072\n",
      "Epoch [15/50] Batch 200/782                  Loss D: 0.0662, loss G: 0.8540\n",
      "Epoch [15/50] Batch 300/782                  Loss D: 0.0893, loss G: 0.7388\n",
      "Epoch [15/50] Batch 400/782                  Loss D: 0.0990, loss G: 1.0128\n",
      "Epoch [15/50] Batch 500/782                  Loss D: 0.0794, loss G: 0.7810\n",
      "Epoch [15/50] Batch 600/782                  Loss D: 0.0646, loss G: 1.0405\n",
      "Epoch [15/50] Batch 700/782                  Loss D: 0.0908, loss G: 0.8067\n",
      "Epoch [16/50] Batch 0/782                  Loss D: 0.0731, loss G: 1.0334\n",
      "Epoch [16/50] Batch 100/782                  Loss D: 0.0704, loss G: 0.9657\n",
      "Epoch [16/50] Batch 200/782                  Loss D: 0.0914, loss G: 1.0900\n",
      "Epoch [16/50] Batch 300/782                  Loss D: 0.0980, loss G: 0.9441\n",
      "Epoch [16/50] Batch 400/782                  Loss D: 0.0623, loss G: 0.8408\n",
      "Epoch [16/50] Batch 500/782                  Loss D: 0.0890, loss G: 0.8081\n",
      "Epoch [16/50] Batch 600/782                  Loss D: 0.0822, loss G: 0.7845\n",
      "Epoch [16/50] Batch 700/782                  Loss D: 0.0796, loss G: 0.7493\n",
      "Epoch [17/50] Batch 0/782                  Loss D: 0.0806, loss G: 0.7372\n",
      "Epoch [17/50] Batch 100/782                  Loss D: 0.0871, loss G: 0.9739\n",
      "Epoch [17/50] Batch 200/782                  Loss D: 0.0893, loss G: 1.2095\n",
      "Epoch [17/50] Batch 300/782                  Loss D: 0.1260, loss G: 0.6002\n",
      "Epoch [17/50] Batch 400/782                  Loss D: 0.0710, loss G: 0.8350\n",
      "Epoch [17/50] Batch 500/782                  Loss D: 0.0977, loss G: 0.8062\n",
      "Epoch [17/50] Batch 600/782                  Loss D: 0.0517, loss G: 0.9889\n",
      "Epoch [17/50] Batch 700/782                  Loss D: 0.1210, loss G: 0.7260\n",
      "Epoch [18/50] Batch 0/782                  Loss D: 0.1883, loss G: 0.5716\n",
      "Epoch [18/50] Batch 100/782                  Loss D: 0.0874, loss G: 0.6875\n",
      "Epoch [18/50] Batch 200/782                  Loss D: 0.1210, loss G: 0.6510\n",
      "Epoch [18/50] Batch 300/782                  Loss D: 0.0834, loss G: 0.8340\n",
      "Epoch [18/50] Batch 400/782                  Loss D: 0.0984, loss G: 0.8925\n",
      "Epoch [18/50] Batch 500/782                  Loss D: 0.1366, loss G: 0.6433\n",
      "Epoch [18/50] Batch 600/782                  Loss D: 0.0856, loss G: 0.6402\n",
      "Epoch [18/50] Batch 700/782                  Loss D: 0.1108, loss G: 0.7783\n",
      "Epoch [19/50] Batch 0/782                  Loss D: 0.0896, loss G: 0.8198\n",
      "Epoch [19/50] Batch 100/782                  Loss D: 0.1316, loss G: 0.6597\n",
      "Epoch [19/50] Batch 200/782                  Loss D: 0.0696, loss G: 0.7739\n",
      "Epoch [19/50] Batch 300/782                  Loss D: 0.0727, loss G: 0.9501\n",
      "Epoch [19/50] Batch 400/782                  Loss D: 0.1018, loss G: 0.8285\n",
      "Epoch [19/50] Batch 500/782                  Loss D: 0.0659, loss G: 0.9321\n",
      "Epoch [19/50] Batch 600/782                  Loss D: 0.0684, loss G: 0.8307\n",
      "Epoch [19/50] Batch 700/782                  Loss D: 0.0753, loss G: 0.9103\n",
      "Epoch [20/50] Batch 0/782                  Loss D: 0.0784, loss G: 0.7986\n",
      "Epoch [20/50] Batch 100/782                  Loss D: 0.0712, loss G: 0.7989\n",
      "Epoch [20/50] Batch 200/782                  Loss D: 0.0751, loss G: 1.1217\n",
      "Epoch [20/50] Batch 300/782                  Loss D: 0.0851, loss G: 1.0560\n",
      "Epoch [20/50] Batch 400/782                  Loss D: 0.0705, loss G: 0.6942\n",
      "Epoch [20/50] Batch 500/782                  Loss D: 0.0854, loss G: 0.8933\n",
      "Epoch [20/50] Batch 600/782                  Loss D: 0.0885, loss G: 0.8477\n",
      "Epoch [20/50] Batch 700/782                  Loss D: 0.0809, loss G: 0.6999\n",
      "Epoch [21/50] Batch 0/782                  Loss D: 0.0633, loss G: 0.6963\n",
      "Epoch [21/50] Batch 100/782                  Loss D: 0.1105, loss G: 0.7151\n",
      "Epoch [21/50] Batch 200/782                  Loss D: 0.1006, loss G: 0.8773\n",
      "Epoch [21/50] Batch 300/782                  Loss D: 0.0860, loss G: 0.8542\n",
      "Epoch [21/50] Batch 400/782                  Loss D: 0.0742, loss G: 0.8289\n",
      "Epoch [21/50] Batch 500/782                  Loss D: 0.0508, loss G: 0.9557\n",
      "Epoch [21/50] Batch 600/782                  Loss D: 0.0911, loss G: 0.8207\n",
      "Epoch [21/50] Batch 700/782                  Loss D: 0.1322, loss G: 0.6233\n",
      "Epoch [22/50] Batch 0/782                  Loss D: 0.0904, loss G: 0.8960\n",
      "Epoch [22/50] Batch 100/782                  Loss D: 0.0617, loss G: 1.0133\n",
      "Epoch [22/50] Batch 200/782                  Loss D: 0.0859, loss G: 0.5833\n",
      "Epoch [22/50] Batch 300/782                  Loss D: 0.0614, loss G: 0.9814\n",
      "Epoch [22/50] Batch 400/782                  Loss D: 0.1290, loss G: 0.5200\n",
      "Epoch [22/50] Batch 500/782                  Loss D: 0.0599, loss G: 0.9282\n",
      "Epoch [22/50] Batch 600/782                  Loss D: 0.0875, loss G: 0.8213\n",
      "Epoch [22/50] Batch 700/782                  Loss D: 0.1433, loss G: 0.6749\n",
      "Epoch [23/50] Batch 0/782                  Loss D: 0.0982, loss G: 0.8204\n",
      "Epoch [23/50] Batch 100/782                  Loss D: 0.1053, loss G: 0.8151\n",
      "Epoch [23/50] Batch 200/782                  Loss D: 0.1078, loss G: 0.7512\n",
      "Epoch [23/50] Batch 300/782                  Loss D: 0.1081, loss G: 0.7679\n",
      "Epoch [23/50] Batch 400/782                  Loss D: 0.0751, loss G: 0.8229\n",
      "Epoch [23/50] Batch 500/782                  Loss D: 0.1089, loss G: 0.7517\n",
      "Epoch [23/50] Batch 600/782                  Loss D: 0.0898, loss G: 0.7935\n",
      "Epoch [23/50] Batch 700/782                  Loss D: 0.0796, loss G: 0.9570\n",
      "Epoch [24/50] Batch 0/782                  Loss D: 0.0994, loss G: 0.7132\n",
      "Epoch [24/50] Batch 100/782                  Loss D: 0.0864, loss G: 0.8238\n",
      "Epoch [24/50] Batch 200/782                  Loss D: 0.0709, loss G: 0.7824\n",
      "Epoch [24/50] Batch 300/782                  Loss D: 0.0963, loss G: 0.6598\n",
      "Epoch [24/50] Batch 400/782                  Loss D: 0.1054, loss G: 0.9174\n",
      "Epoch [24/50] Batch 500/782                  Loss D: 0.0671, loss G: 0.5402\n",
      "Epoch [24/50] Batch 600/782                  Loss D: 0.1392, loss G: 0.5976\n",
      "Epoch [24/50] Batch 700/782                  Loss D: 0.0942, loss G: 0.8268\n",
      "Epoch [25/50] Batch 0/782                  Loss D: 0.0745, loss G: 0.8239\n",
      "Epoch [25/50] Batch 100/782                  Loss D: 0.0864, loss G: 0.7201\n",
      "Epoch [25/50] Batch 200/782                  Loss D: 0.0871, loss G: 0.7842\n",
      "Epoch [25/50] Batch 300/782                  Loss D: 0.1099, loss G: 0.6322\n",
      "Epoch [25/50] Batch 400/782                  Loss D: 0.1323, loss G: 0.8264\n",
      "Epoch [25/50] Batch 500/782                  Loss D: 0.1340, loss G: 0.7437\n",
      "Epoch [25/50] Batch 600/782                  Loss D: 0.0851, loss G: 0.7867\n",
      "Epoch [25/50] Batch 700/782                  Loss D: 0.1785, loss G: 0.5560\n",
      "Epoch [26/50] Batch 0/782                  Loss D: 0.0932, loss G: 0.7355\n",
      "Epoch [26/50] Batch 100/782                  Loss D: 0.0887, loss G: 0.6461\n",
      "Epoch [26/50] Batch 200/782                  Loss D: 0.1345, loss G: 0.5177\n",
      "Epoch [26/50] Batch 300/782                  Loss D: 0.1717, loss G: 0.7834\n",
      "Epoch [26/50] Batch 400/782                  Loss D: 0.1049, loss G: 0.7209\n",
      "Epoch [26/50] Batch 500/782                  Loss D: 0.1279, loss G: 0.5507\n",
      "Epoch [26/50] Batch 600/782                  Loss D: 0.0958, loss G: 0.6484\n",
      "Epoch [26/50] Batch 700/782                  Loss D: 0.0966, loss G: 1.0313\n",
      "Epoch [27/50] Batch 0/782                  Loss D: 0.1129, loss G: 0.7126\n",
      "Epoch [27/50] Batch 100/782                  Loss D: 0.0924, loss G: 0.8276\n",
      "Epoch [27/50] Batch 200/782                  Loss D: 0.1242, loss G: 0.6397\n",
      "Epoch [27/50] Batch 300/782                  Loss D: 0.1579, loss G: 0.6155\n",
      "Epoch [27/50] Batch 400/782                  Loss D: 0.1403, loss G: 0.6369\n",
      "Epoch [27/50] Batch 500/782                  Loss D: 0.1456, loss G: 0.7542\n",
      "Epoch [27/50] Batch 600/782                  Loss D: 0.0945, loss G: 0.7454\n",
      "Epoch [27/50] Batch 700/782                  Loss D: 0.1373, loss G: 0.5552\n",
      "Epoch [28/50] Batch 0/782                  Loss D: 0.1276, loss G: 0.5179\n",
      "Epoch [28/50] Batch 100/782                  Loss D: 0.1283, loss G: 0.5009\n",
      "Epoch [28/50] Batch 200/782                  Loss D: 0.1171, loss G: 0.7753\n",
      "Epoch [28/50] Batch 300/782                  Loss D: 0.1458, loss G: 0.6378\n",
      "Epoch [28/50] Batch 400/782                  Loss D: 0.1243, loss G: 0.8358\n",
      "Epoch [28/50] Batch 500/782                  Loss D: 0.1337, loss G: 0.6298\n",
      "Epoch [28/50] Batch 600/782                  Loss D: 0.1556, loss G: 0.7933\n",
      "Epoch [28/50] Batch 700/782                  Loss D: 0.1316, loss G: 0.9270\n",
      "Epoch [29/50] Batch 0/782                  Loss D: 0.1088, loss G: 0.6999\n",
      "Epoch [29/50] Batch 100/782                  Loss D: 0.1494, loss G: 0.4968\n",
      "Epoch [29/50] Batch 200/782                  Loss D: 0.1644, loss G: 0.5287\n",
      "Epoch [29/50] Batch 300/782                  Loss D: 0.1684, loss G: 0.5146\n",
      "Epoch [29/50] Batch 400/782                  Loss D: 0.1486, loss G: 0.5998\n",
      "Epoch [29/50] Batch 500/782                  Loss D: 0.1376, loss G: 0.7568\n",
      "Epoch [29/50] Batch 600/782                  Loss D: 0.1720, loss G: 0.5762\n",
      "Epoch [29/50] Batch 700/782                  Loss D: 0.1394, loss G: 0.6724\n",
      "Epoch [30/50] Batch 0/782                  Loss D: 0.1102, loss G: 0.7100\n",
      "Epoch [30/50] Batch 100/782                  Loss D: 0.1254, loss G: 0.6913\n",
      "Epoch [30/50] Batch 200/782                  Loss D: 0.1686, loss G: 0.7027\n",
      "Epoch [30/50] Batch 300/782                  Loss D: 0.1225, loss G: 0.5397\n",
      "Epoch [30/50] Batch 400/782                  Loss D: 0.1086, loss G: 0.6677\n",
      "Epoch [30/50] Batch 500/782                  Loss D: 0.1386, loss G: 0.8154\n",
      "Epoch [30/50] Batch 600/782                  Loss D: 0.1410, loss G: 0.6486\n",
      "Epoch [30/50] Batch 700/782                  Loss D: 0.0940, loss G: 0.7805\n",
      "Epoch [31/50] Batch 0/782                  Loss D: 0.1334, loss G: 0.8349\n",
      "Epoch [31/50] Batch 100/782                  Loss D: 0.1541, loss G: 0.5652\n",
      "Epoch [31/50] Batch 200/782                  Loss D: 0.1680, loss G: 0.5807\n",
      "Epoch [31/50] Batch 300/782                  Loss D: 0.1718, loss G: 0.6239\n",
      "Epoch [31/50] Batch 400/782                  Loss D: 0.1979, loss G: 0.5653\n",
      "Epoch [31/50] Batch 500/782                  Loss D: 0.2142, loss G: 0.4775\n",
      "Epoch [31/50] Batch 600/782                  Loss D: 0.1217, loss G: 0.6842\n",
      "Epoch [31/50] Batch 700/782                  Loss D: 0.1446, loss G: 0.6410\n",
      "Epoch [32/50] Batch 0/782                  Loss D: 0.1205, loss G: 0.7765\n",
      "Epoch [32/50] Batch 100/782                  Loss D: 0.1384, loss G: 0.6430\n",
      "Epoch [32/50] Batch 200/782                  Loss D: 0.1427, loss G: 0.7825\n",
      "Epoch [32/50] Batch 300/782                  Loss D: 0.1858, loss G: 0.5860\n",
      "Epoch [32/50] Batch 400/782                  Loss D: 0.1599, loss G: 0.5828\n",
      "Epoch [32/50] Batch 500/782                  Loss D: 0.1860, loss G: 0.4879\n",
      "Epoch [32/50] Batch 600/782                  Loss D: 0.1223, loss G: 0.6634\n",
      "Epoch [32/50] Batch 700/782                  Loss D: 0.1400, loss G: 0.6130\n",
      "Epoch [33/50] Batch 0/782                  Loss D: 0.1485, loss G: 0.6601\n",
      "Epoch [33/50] Batch 100/782                  Loss D: 0.1286, loss G: 0.5433\n",
      "Epoch [33/50] Batch 200/782                  Loss D: 0.2148, loss G: 0.5351\n",
      "Epoch [33/50] Batch 300/782                  Loss D: 0.1475, loss G: 0.8133\n",
      "Epoch [33/50] Batch 400/782                  Loss D: 0.1408, loss G: 0.6289\n",
      "Epoch [33/50] Batch 500/782                  Loss D: 0.1402, loss G: 0.6369\n",
      "Epoch [33/50] Batch 600/782                  Loss D: 0.1673, loss G: 0.5975\n",
      "Epoch [33/50] Batch 700/782                  Loss D: 0.1515, loss G: 0.7071\n",
      "Epoch [34/50] Batch 0/782                  Loss D: 0.1387, loss G: 0.5471\n",
      "Epoch [34/50] Batch 100/782                  Loss D: 0.1787, loss G: 0.4900\n",
      "Epoch [34/50] Batch 200/782                  Loss D: 0.1396, loss G: 0.7991\n",
      "Epoch [34/50] Batch 300/782                  Loss D: 0.1538, loss G: 0.6531\n",
      "Epoch [34/50] Batch 400/782                  Loss D: 0.1831, loss G: 0.5628\n",
      "Epoch [34/50] Batch 500/782                  Loss D: 0.1619, loss G: 0.6111\n",
      "Epoch [34/50] Batch 600/782                  Loss D: 0.1394, loss G: 0.6190\n",
      "Epoch [34/50] Batch 700/782                  Loss D: 0.1715, loss G: 0.3895\n",
      "Epoch [35/50] Batch 0/782                  Loss D: 0.2110, loss G: 0.4369\n",
      "Epoch [35/50] Batch 100/782                  Loss D: 0.1512, loss G: 0.5990\n",
      "Epoch [35/50] Batch 200/782                  Loss D: 0.1570, loss G: 0.5472\n",
      "Epoch [35/50] Batch 300/782                  Loss D: 0.1688, loss G: 0.5250\n",
      "Epoch [35/50] Batch 400/782                  Loss D: 0.1310, loss G: 0.5311\n",
      "Epoch [35/50] Batch 500/782                  Loss D: 0.1685, loss G: 0.5492\n",
      "Epoch [35/50] Batch 600/782                  Loss D: 0.1916, loss G: 0.6372\n",
      "Epoch [35/50] Batch 700/782                  Loss D: 0.1365, loss G: 0.5325\n",
      "Epoch [36/50] Batch 0/782                  Loss D: 0.1756, loss G: 0.6385\n",
      "Epoch [36/50] Batch 100/782                  Loss D: 0.1792, loss G: 0.5507\n",
      "Epoch [36/50] Batch 200/782                  Loss D: 0.1705, loss G: 0.4790\n",
      "Epoch [36/50] Batch 300/782                  Loss D: 0.2014, loss G: 0.5430\n",
      "Epoch [36/50] Batch 400/782                  Loss D: 0.1915, loss G: 0.4199\n",
      "Epoch [36/50] Batch 500/782                  Loss D: 0.1781, loss G: 0.5970\n",
      "Epoch [36/50] Batch 600/782                  Loss D: 0.1614, loss G: 0.6754\n",
      "Epoch [36/50] Batch 700/782                  Loss D: 0.1844, loss G: 0.5825\n",
      "Epoch [37/50] Batch 0/782                  Loss D: 0.1853, loss G: 0.5328\n",
      "Epoch [37/50] Batch 100/782                  Loss D: 0.1742, loss G: 0.6492\n",
      "Epoch [37/50] Batch 200/782                  Loss D: 0.1609, loss G: 0.4657\n",
      "Epoch [37/50] Batch 300/782                  Loss D: 0.1282, loss G: 0.5768\n",
      "Epoch [37/50] Batch 400/782                  Loss D: 0.1953, loss G: 0.3596\n",
      "Epoch [37/50] Batch 500/782                  Loss D: 0.1548, loss G: 0.6448\n",
      "Epoch [37/50] Batch 600/782                  Loss D: 0.1610, loss G: 0.5112\n",
      "Epoch [37/50] Batch 700/782                  Loss D: 0.1544, loss G: 0.6855\n",
      "Epoch [38/50] Batch 0/782                  Loss D: 0.1494, loss G: 0.6681\n",
      "Epoch [38/50] Batch 100/782                  Loss D: 0.1632, loss G: 0.6007\n",
      "Epoch [38/50] Batch 200/782                  Loss D: 0.1563, loss G: 0.6597\n",
      "Epoch [38/50] Batch 300/782                  Loss D: 0.1537, loss G: 0.5591\n",
      "Epoch [38/50] Batch 400/782                  Loss D: 0.1466, loss G: 0.7327\n",
      "Epoch [38/50] Batch 500/782                  Loss D: 0.1495, loss G: 0.4387\n",
      "Epoch [38/50] Batch 600/782                  Loss D: 0.2184, loss G: 0.3163\n",
      "Epoch [38/50] Batch 700/782                  Loss D: 0.1414, loss G: 0.6311\n",
      "Epoch [39/50] Batch 0/782                  Loss D: 0.1489, loss G: 0.6552\n",
      "Epoch [39/50] Batch 100/782                  Loss D: 0.1635, loss G: 0.4943\n",
      "Epoch [39/50] Batch 200/782                  Loss D: 0.2009, loss G: 0.4263\n",
      "Epoch [39/50] Batch 300/782                  Loss D: 0.2029, loss G: 0.7708\n",
      "Epoch [39/50] Batch 400/782                  Loss D: 0.1477, loss G: 0.5824\n",
      "Epoch [39/50] Batch 500/782                  Loss D: 0.2015, loss G: 0.3979\n",
      "Epoch [39/50] Batch 600/782                  Loss D: 0.1513, loss G: 0.5357\n",
      "Epoch [39/50] Batch 700/782                  Loss D: 0.1666, loss G: 0.4981\n",
      "Epoch [40/50] Batch 0/782                  Loss D: 0.1345, loss G: 0.7604\n",
      "Epoch [40/50] Batch 100/782                  Loss D: 0.1863, loss G: 0.4809\n",
      "Epoch [40/50] Batch 200/782                  Loss D: 0.1720, loss G: 0.6567\n",
      "Epoch [40/50] Batch 300/782                  Loss D: 0.1486, loss G: 0.6252\n",
      "Epoch [40/50] Batch 400/782                  Loss D: 0.1554, loss G: 0.4829\n",
      "Epoch [40/50] Batch 500/782                  Loss D: 0.1581, loss G: 0.6383\n",
      "Epoch [40/50] Batch 600/782                  Loss D: 0.1506, loss G: 0.5565\n",
      "Epoch [40/50] Batch 700/782                  Loss D: 0.1842, loss G: 0.4561\n",
      "Epoch [41/50] Batch 0/782                  Loss D: 0.2067, loss G: 0.4121\n",
      "Epoch [41/50] Batch 100/782                  Loss D: 0.1837, loss G: 0.4785\n",
      "Epoch [41/50] Batch 200/782                  Loss D: 0.1574, loss G: 0.8563\n",
      "Epoch [41/50] Batch 300/782                  Loss D: 0.1731, loss G: 0.4649\n",
      "Epoch [41/50] Batch 400/782                  Loss D: 0.1383, loss G: 0.5264\n",
      "Epoch [41/50] Batch 500/782                  Loss D: 0.1482, loss G: 0.4894\n",
      "Epoch [41/50] Batch 600/782                  Loss D: 0.1606, loss G: 0.5696\n",
      "Epoch [41/50] Batch 700/782                  Loss D: 0.2300, loss G: 0.4020\n",
      "Epoch [42/50] Batch 0/782                  Loss D: 0.2373, loss G: 0.3310\n",
      "Epoch [42/50] Batch 100/782                  Loss D: 0.1923, loss G: 0.4198\n",
      "Epoch [42/50] Batch 200/782                  Loss D: 0.1506, loss G: 0.5328\n",
      "Epoch [42/50] Batch 300/782                  Loss D: 0.1567, loss G: 0.5223\n",
      "Epoch [42/50] Batch 400/782                  Loss D: 0.1746, loss G: 0.6548\n",
      "Epoch [42/50] Batch 500/782                  Loss D: 0.1978, loss G: 0.5704\n",
      "Epoch [42/50] Batch 600/782                  Loss D: 0.1932, loss G: 0.4943\n",
      "Epoch [42/50] Batch 700/782                  Loss D: 0.1927, loss G: 0.7061\n",
      "Epoch [43/50] Batch 0/782                  Loss D: 0.1801, loss G: 0.4225\n",
      "Epoch [43/50] Batch 100/782                  Loss D: 0.1751, loss G: 0.6202\n",
      "Epoch [43/50] Batch 200/782                  Loss D: 0.1821, loss G: 0.5639\n",
      "Epoch [43/50] Batch 300/782                  Loss D: 0.2023, loss G: 0.5990\n",
      "Epoch [43/50] Batch 400/782                  Loss D: 0.1753, loss G: 0.3355\n",
      "Epoch [43/50] Batch 500/782                  Loss D: 0.2221, loss G: 0.5350\n",
      "Epoch [43/50] Batch 600/782                  Loss D: 0.1574, loss G: 0.4515\n",
      "Epoch [43/50] Batch 700/782                  Loss D: 0.2371, loss G: 0.5427\n",
      "Epoch [44/50] Batch 0/782                  Loss D: 0.1462, loss G: 0.5807\n",
      "Epoch [44/50] Batch 100/782                  Loss D: 0.1607, loss G: 0.4343\n",
      "Epoch [44/50] Batch 200/782                  Loss D: 0.2172, loss G: 0.4417\n",
      "Epoch [44/50] Batch 300/782                  Loss D: 0.2014, loss G: 0.5109\n",
      "Epoch [44/50] Batch 400/782                  Loss D: 0.2245, loss G: 0.4264\n",
      "Epoch [44/50] Batch 500/782                  Loss D: 0.1639, loss G: 0.4529\n",
      "Epoch [44/50] Batch 600/782                  Loss D: 0.1836, loss G: 0.4788\n",
      "Epoch [44/50] Batch 700/782                  Loss D: 0.1718, loss G: 0.5513\n",
      "Epoch [45/50] Batch 0/782                  Loss D: 0.1893, loss G: 0.5905\n",
      "Epoch [45/50] Batch 100/782                  Loss D: 0.1627, loss G: 0.5086\n",
      "Epoch [45/50] Batch 200/782                  Loss D: 0.1944, loss G: 0.4383\n",
      "Epoch [45/50] Batch 300/782                  Loss D: 0.1262, loss G: 0.4881\n",
      "Epoch [45/50] Batch 400/782                  Loss D: 0.1764, loss G: 0.5309\n",
      "Epoch [45/50] Batch 500/782                  Loss D: 0.2080, loss G: 0.3830\n",
      "Epoch [45/50] Batch 600/782                  Loss D: 0.1996, loss G: 0.5193\n",
      "Epoch [45/50] Batch 700/782                  Loss D: 0.1680, loss G: 0.5254\n",
      "Epoch [46/50] Batch 0/782                  Loss D: 0.2079, loss G: 0.5139\n",
      "Epoch [46/50] Batch 100/782                  Loss D: 0.1957, loss G: 0.5597\n",
      "Epoch [46/50] Batch 200/782                  Loss D: 0.1535, loss G: 0.4776\n",
      "Epoch [46/50] Batch 300/782                  Loss D: 0.1972, loss G: 0.4055\n",
      "Epoch [46/50] Batch 400/782                  Loss D: 0.1871, loss G: 0.4299\n",
      "Epoch [46/50] Batch 500/782                  Loss D: 0.1716, loss G: 0.3543\n",
      "Epoch [46/50] Batch 600/782                  Loss D: 0.1946, loss G: 0.4759\n",
      "Epoch [46/50] Batch 700/782                  Loss D: 0.2072, loss G: 0.3850\n",
      "Epoch [47/50] Batch 0/782                  Loss D: 0.2243, loss G: 0.4704\n",
      "Epoch [47/50] Batch 100/782                  Loss D: 0.1832, loss G: 0.4294\n",
      "Epoch [47/50] Batch 200/782                  Loss D: 0.2017, loss G: 0.4799\n",
      "Epoch [47/50] Batch 300/782                  Loss D: 0.2414, loss G: 0.4799\n",
      "Epoch [47/50] Batch 400/782                  Loss D: 0.2428, loss G: 0.4351\n",
      "Epoch [47/50] Batch 500/782                  Loss D: 0.1956, loss G: 0.4178\n",
      "Epoch [47/50] Batch 600/782                  Loss D: 0.2178, loss G: 0.6092\n",
      "Epoch [47/50] Batch 700/782                  Loss D: 0.1995, loss G: 0.5296\n",
      "Epoch [48/50] Batch 0/782                  Loss D: 0.2177, loss G: 0.4605\n",
      "Epoch [48/50] Batch 100/782                  Loss D: 0.2445, loss G: 0.3833\n",
      "Epoch [48/50] Batch 200/782                  Loss D: 0.2313, loss G: 0.3050\n",
      "Epoch [48/50] Batch 300/782                  Loss D: 0.2181, loss G: 0.4983\n",
      "Epoch [48/50] Batch 400/782                  Loss D: 0.2100, loss G: 0.3369\n",
      "Epoch [48/50] Batch 500/782                  Loss D: 0.1717, loss G: 0.6491\n",
      "Epoch [48/50] Batch 600/782                  Loss D: 0.2166, loss G: 0.7421\n",
      "Epoch [48/50] Batch 700/782                  Loss D: 0.2062, loss G: 0.4134\n",
      "Epoch [49/50] Batch 0/782                  Loss D: 0.2202, loss G: 0.3590\n",
      "Epoch [49/50] Batch 100/782                  Loss D: 0.1430, loss G: 0.6882\n",
      "Epoch [49/50] Batch 200/782                  Loss D: 0.1582, loss G: 0.6064\n",
      "Epoch [49/50] Batch 300/782                  Loss D: 0.2183, loss G: 0.4802\n",
      "Epoch [49/50] Batch 400/782                  Loss D: 0.1881, loss G: 0.4455\n",
      "Epoch [49/50] Batch 500/782                  Loss D: 0.1632, loss G: 0.4612\n",
      "Epoch [49/50] Batch 600/782                  Loss D: 0.1611, loss G: 0.5182\n",
      "Epoch [49/50] Batch 700/782                  Loss D: 0.3242, loss G: 0.2690\n"
     ]
    }
   ],
   "source": [
    "# GAN archicture trial\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, data in enumerate(dataloader_one):\n",
    "        worker = workers[0]\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "        fake = main_server.generator(noise)\n",
    "        real, _ = data\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        current_disc_real = worker.discriminator(real).reshape(-1)\n",
    "        worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "        current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "        worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "        worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "        worker.discriminator.zero_grad()\n",
    "        worker.loss_disc.backward()\n",
    "        worker.d_optimizer.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = worker.discriminator(fake).reshape(-1)\n",
    "        main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "        main_server.generator.zero_grad()\n",
    "        main_server.loss_gen.backward()\n",
    "        main_server.g_optimizer.step()\n",
    "\n",
    "        logger.log(worker.loss_disc.item(),main_server.loss_gen.item(),worker.loss_disc_real, worker.loss_disc_fake,epoch,i,len(dataloader_one))\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {i}/{len(dataloader_one)} \\\n",
    "                 Loss D: {worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}\"\n",
    "            )\n",
    "        if i% 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = main_server.generator(fixed_noise)\n",
    "                logger.log_images(fake,BATCH_SIZE, epoch, i, len(dataloader_one))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/782                  Loss D: 0.1942, loss G: 0.4418\n",
      "Epoch [0/50] Batch 100/782                  Loss D: 0.1971, loss G: 0.4455\n",
      "Epoch [0/50] Batch 200/782                  Loss D: 0.1793, loss G: 0.3768\n",
      "Epoch [0/50] Batch 300/782                  Loss D: 0.1976, loss G: 0.5090\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bshqga\\OneDrive - Scania CV\\code\\F2U\\main.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m fake \u001b[39m=\u001b[39m main_server\u001b[39m.\u001b[39mgenerator(noise)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m worker_id, worker \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(workers):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     current_worker_real \u001b[39m=\u001b[39m worker_loaders[worker_id][batch_id]\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mto(dev)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# print(real.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     current_disc_real \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39mdiscriminator(current_worker_real)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main training loop for F2U\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_id in range(len(worker_loaders[0])):\n",
    "\n",
    "        highest_loss = 0\n",
    "        chosen_discriminator = None\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "        fake = main_server.generator(noise)\n",
    "\n",
    "        for worker_id, worker in enumerate(workers):\n",
    "            current_worker_real = worker_loaders[worker_id][batch_id].float().to(dev)\n",
    "            # print(real.shape)\n",
    "\n",
    "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            current_disc_real = worker.discriminator(current_worker_real).reshape(-1)\n",
    "            worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "            current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "            worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "            worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "            worker.discriminator.zero_grad()\n",
    "            worker.loss_disc.backward()\n",
    "            worker.d_optimizer.step()\n",
    "            # print(worker.loss_disc_fake, i)\n",
    "            if highest_loss < worker.loss_disc_fake:\n",
    "                highest_loss = worker.loss_disc_fake\n",
    "                chosen_discriminator = worker_id\n",
    "        # print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item():.4f}\")\n",
    "        chosen_worker = workers[chosen_discriminator]\n",
    "        \n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = chosen_worker.discriminator(fake).reshape(-1)\n",
    "        main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "        main_server.generator.zero_grad()\n",
    "        main_server.loss_gen.backward()\n",
    "        main_server.g_optimizer.step()\n",
    "\n",
    "        logger.log(chosen_worker.loss_disc.item(),main_server.loss_gen.item(),chosen_worker.loss_disc_real, chosen_worker.loss_disc_fake,epoch,i,len(worker_loaders[0]))\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(worker_loaders[0])} \\\n",
    "                 Loss D: {worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = main_server.generator(noise)\n",
    "                logger.log_images(fake,BATCH_SIZE, epoch, i, len(dataloader_one))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = []\n",
    "\n",
    "# for worker in workers:\n",
    "#     # print(worker.x_data.shape)\n",
    "#     dataloaders.append(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE))\n",
    "\n",
    "# i = iter(dataloaders[0])\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# NOISE_DIM = 128\n",
    "# fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(dev)\n",
    "# writer_real = SummaryWriter(f\"logs/real\")\n",
    "# writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "# step = 0\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     highest_loss = 0\n",
    "#     chosen_discriminator = None\n",
    "#     for i, worker in enumerate(workers):\n",
    "#         print(worker.x_data.shape)\n",
    "#         dataloader = DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)\n",
    "#         for batch_id, real in enumerate(dataloader):\n",
    "#             real = real.float().to(dev)\n",
    "#             # print(real.shape)\n",
    "#             # print(real)\n",
    "#             noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "#             fake = main_server.generator(noise)\n",
    "\n",
    "#             ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "#             disc_real = worker.discriminator(real).reshape(-1)\n",
    "#             worker.loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "#             disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "#             worker.loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "#             loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "#             worker.discriminator.zero_grad()\n",
    "#             loss_disc.backward()\n",
    "#             worker.d_optimizer.step()\n",
    "#             if batch_id % 20 == 0:\n",
    "#                 print(\n",
    "#                     f\"Worker: {i} Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(dataloader)} \\\n",
    "#                         Loss D: {loss_disc:.4f}\"\n",
    "#                 )\n",
    "#         # print(worker.loss_disc_fake, i)\n",
    "#         if highest_loss < worker.loss_disc_fake:\n",
    "#             highest_loss = worker.loss_disc_fake\n",
    "#             chosen_discriminator = i\n",
    "#         print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item()}\")\n",
    "#     dataloader = DataLoader(dataset=workers[chosen_discriminator].x_data,batch_size=BATCH_SIZE)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "    # output = disc(fake).reshape(-1)\n",
    "    # loss_gen = criterion(output, torch.ones_like(output))\n",
    "    # gen.zero_grad()\n",
    "    # loss_gen.backward()\n",
    "    # opt_gen.step()\n",
    "\n",
    "    # for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    #     real = real.to(device)\n",
    "    #     noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "    #     fake = gen(noise)\n",
    "\n",
    "    #     ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "    #     disc_real = disc(real).reshape(-1)\n",
    "    #     loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "    #     disc_fake = disc(fake.detach()).reshape(-1)\n",
    "    #     loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "    #     loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "    #     disc.zero_grad()\n",
    "    #     loss_disc.backward()\n",
    "    #     opt_disc.step()\n",
    "\n",
    "    #     ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "    #     output = disc(fake).reshape(-1)\n",
    "    #     loss_gen = criterion(output, torch.ones_like(output))\n",
    "    #     gen.zero_grad()\n",
    "    #     loss_gen.backward()\n",
    "    #     opt_gen.step()\n",
    "\n",
    "    #     # Print losses occasionally and print to tensorboard\n",
    "    #     if batch_idx % 100 == 0:\n",
    "    #         print(\n",
    "    #             f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "    #               Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "    #         )\n",
    "\n",
    "    #         with torch.no_grad():\n",
    "    #             fake = gen(fixed_noise)\n",
    "    #             # take out (up to) 32 examples\n",
    "    #             img_grid_real = torchvision.utils.make_grid(\n",
    "    #                 real[:32], normalize=True\n",
    "    #             )\n",
    "    #             img_grid_fake = torchvision.utils.make_grid(\n",
    "    #                 fake[:32], normalize=True\n",
    "    #             )\n",
    "\n",
    "    #             writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "    #             writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "    #         step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
