{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from dataTransformation import labels4clients, distribute_data_labels4clients\n",
    "from gan_model import Discriminator, Generator, initialize_weights\n",
    "from network import Server, Worker\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS_IMG = 3\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_DISC = 32\n",
    "FEATURES_GEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 5\n",
    "num_unique_users = num_workers\n",
    "num_classes = 10\n",
    "classes_per_user = 4\n",
    "\n",
    "learning_rate = 0.0002\n",
    "test_samples_num = 16\n",
    "epochs_num = 200\n",
    "\n",
    "dictionary = labels4clients(num_classes,classes_per_user,num_workers,num_unique_users,False)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.CIFAR10(root='./datasets/cifar/', train=True, download=True, transform=trans_cifar)\n",
    "dataset_test = datasets.CIFAR10(root='./datasets/cifar/', train=False, download=True, transform=trans_cifar)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.data[0])\n",
    "print(dataset.transforms(dataset.data[0],transforms.ToTensor()))\n",
    "print(dataset.transforms(dataset.data[0],trans_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "print(normalized)\n",
    "trans_cifar(dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.data.shape)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = dataloader.dataset[0]\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np = np.empty((dataset.data.shape[0],dataset.data.shape[3],dataset.data.shape[1],dataset.data.shape[2]))\n",
    "for i in range(len(dataset)):\n",
    "    x_train_normalized_np[i] = trans_cifar(dataset.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(dataset.data)\n",
    "y_train = np.asarray(dataset.targets)\n",
    "x_clinet_list, y_client_list = distribute_data_labels4clients(x_train_normalized_np,y_train,dictionary,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDist(y,num_classes,user_num):\n",
    "    ax = sns.countplot(x=y)\n",
    "    ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(x_clinet_list)):\n",
    "    print(len(y_client_list[i]))\n",
    "    getDist(y_client_list[i],num_classes,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.main = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(in_channels=128, out_channels=512, kernel_size=4, stride=1, padding=0, bias = False),\n",
    "#             nn.BatchNorm2d(num_features=512,momentum=0.1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "#             nn.BatchNorm2d(num_features=256,momentum=0.1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "#             nn.BatchNorm2d(num_features=128,momentum=0.1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "#             nn.BatchNorm2d(num_features=64,momentum=0.1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias = False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         output = self.main(input)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.main = nn.Sequential(\n",
    "#             spectral_norm(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias = False)),\n",
    "#             nn.LeakyReLU(negative_slope= 0.1, inplace = True),\n",
    "\n",
    "#             spectral_norm(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias = False)),\n",
    "#             nn.LeakyReLU(negative_slope= 0.1, inplace = True),\n",
    "\n",
    "#             spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias = False)),\n",
    "#             nn.LeakyReLU(negative_slope= 0.1, inplace = True),\n",
    "\n",
    "#             spectral_norm(nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1, bias = False)),\n",
    "#             nn.LeakyReLU(negative_slope= 0.1, inplace = True),\n",
    "\n",
    "#             # need to calculate the number of neurons in this layer to connect each of their outputs to the next layer\n",
    "#             spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias = False)),\n",
    "#             nn.LeakyReLU(negative_slope= 0.1, inplace = True),\n",
    "#             nn.Flatten(), #flatten the output\n",
    "#             spectral_norm(nn.Linear(in_features =4096,out_features =1, bias = False))\n",
    "#         )\n",
    "\n",
    "#     def neuron_calculator(in_channels,padding,kernel_size,stride,out_channels):\n",
    "#         return (in_channels+2*padding-kernel_size)**2 * out_channels\n",
    "#     def forward(self, input):\n",
    "#         output = self.main(input)\n",
    "#         return output.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "dev = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG = Generator().to(dev)\n",
    "# netD = Discriminator().to(dev)\n",
    "# summary(netG,(128,1,1))\n",
    "# summary(netD,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_server = Server(0,learning_rate)\n",
    "main_server.generator.train()\n",
    "workers = []\n",
    "for i in range(num_workers):\n",
    "    worker = Worker(i,learning_rate)\n",
    "    # x_clinet_list[i] = np.transpose(x_clinet_list[i],(0, 3, 1, 2))\n",
    "    worker.load_worker_data(x_clinet_list[i], y_client_list[i])\n",
    "    worker.discriminator.train()\n",
    "    workers.append(worker)\n",
    "    \n",
    "# summary(main_server.generator,(128,1,1))\n",
    "# summary(workers[0].discriminator,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "NOISE_DIM = 128\n",
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(dev)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0\n",
    "\n",
    "worker_loaders = []\n",
    "\n",
    "for worker in workers:\n",
    "    # print(worker.x_data.shape)\n",
    "    worker_loaders.append([])\n",
    "    for batch_id, real in enumerate(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)):\n",
    "        worker_loaders[-1].append(real)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_id in range(len(worker_loaders[0])):\n",
    "\n",
    "        highest_loss = 0\n",
    "        chosen_discriminator = None\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "        fake = main_server.generator(noise)\n",
    "\n",
    "        for worker_id, worker in enumerate(workers):\n",
    "            current_worker_real = worker_loaders[worker_id][batch_id].float().to(dev)\n",
    "            # print(real.shape)\n",
    "\n",
    "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            current_disc_real = worker.discriminator(current_worker_real).reshape(-1)\n",
    "            worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "            current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "            worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "            worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "            worker.discriminator.zero_grad()\n",
    "            worker.loss_disc.backward()\n",
    "            worker.d_optimizer.step()\n",
    "            # print(worker.loss_disc_fake, i)\n",
    "            if highest_loss < worker.loss_disc_fake:\n",
    "                highest_loss = worker.loss_disc_fake\n",
    "                chosen_discriminator = worker_id\n",
    "        print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item():.4f}\")\n",
    "        chosen_worker = workers[chosen_discriminator]\n",
    "        \n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = chosen_worker.discriminator(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        main_server.generator.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        main_server.g_optimizer.step()\n",
    "\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_id % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(worker_loaders[0])} \\\n",
    "                  chosen D: {chosen_discriminator} Loss D: {chosen_worker.loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = main_server.generator(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = utils.make_grid(\n",
    "                    real[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = []\n",
    "\n",
    "# for worker in workers:\n",
    "#     # print(worker.x_data.shape)\n",
    "#     dataloaders.append(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE))\n",
    "\n",
    "# i = iter(dataloaders[0])\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# NOISE_DIM = 128\n",
    "# fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(dev)\n",
    "# writer_real = SummaryWriter(f\"logs/real\")\n",
    "# writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "# step = 0\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     highest_loss = 0\n",
    "#     chosen_discriminator = None\n",
    "#     for i, worker in enumerate(workers):\n",
    "#         print(worker.x_data.shape)\n",
    "#         dataloader = DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)\n",
    "#         for batch_id, real in enumerate(dataloader):\n",
    "#             real = real.float().to(dev)\n",
    "#             # print(real.shape)\n",
    "#             # print(real)\n",
    "#             noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "#             fake = main_server.generator(noise)\n",
    "\n",
    "#             ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "#             disc_real = worker.discriminator(real).reshape(-1)\n",
    "#             worker.loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "#             disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "#             worker.loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "#             loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "#             worker.discriminator.zero_grad()\n",
    "#             loss_disc.backward()\n",
    "#             worker.d_optimizer.step()\n",
    "#             if batch_id % 20 == 0:\n",
    "#                 print(\n",
    "#                     f\"Worker: {i} Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(dataloader)} \\\n",
    "#                         Loss D: {loss_disc:.4f}\"\n",
    "#                 )\n",
    "#         # print(worker.loss_disc_fake, i)\n",
    "#         if highest_loss < worker.loss_disc_fake:\n",
    "#             highest_loss = worker.loss_disc_fake\n",
    "#             chosen_discriminator = i\n",
    "#         print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item()}\")\n",
    "#     dataloader = DataLoader(dataset=workers[chosen_discriminator].x_data,batch_size=BATCH_SIZE)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "    # output = disc(fake).reshape(-1)\n",
    "    # loss_gen = criterion(output, torch.ones_like(output))\n",
    "    # gen.zero_grad()\n",
    "    # loss_gen.backward()\n",
    "    # opt_gen.step()\n",
    "\n",
    "    # for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    #     real = real.to(device)\n",
    "    #     noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "    #     fake = gen(noise)\n",
    "\n",
    "    #     ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "    #     disc_real = disc(real).reshape(-1)\n",
    "    #     loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "    #     disc_fake = disc(fake.detach()).reshape(-1)\n",
    "    #     loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "    #     loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "    #     disc.zero_grad()\n",
    "    #     loss_disc.backward()\n",
    "    #     opt_disc.step()\n",
    "\n",
    "    #     ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "    #     output = disc(fake).reshape(-1)\n",
    "    #     loss_gen = criterion(output, torch.ones_like(output))\n",
    "    #     gen.zero_grad()\n",
    "    #     loss_gen.backward()\n",
    "    #     opt_gen.step()\n",
    "\n",
    "    #     # Print losses occasionally and print to tensorboard\n",
    "    #     if batch_idx % 100 == 0:\n",
    "    #         print(\n",
    "    #             f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "    #               Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "    #         )\n",
    "\n",
    "    #         with torch.no_grad():\n",
    "    #             fake = gen(fixed_noise)\n",
    "    #             # take out (up to) 32 examples\n",
    "    #             img_grid_real = torchvision.utils.make_grid(\n",
    "    #                 real[:32], normalize=True\n",
    "    #             )\n",
    "    #             img_grid_fake = torchvision.utils.make_grid(\n",
    "    #                 fake[:32], normalize=True\n",
    "    #             )\n",
    "\n",
    "    #             writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "    #             writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "    #         step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
