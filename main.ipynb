{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, utils\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dataTransformation import labels4clients, distribute_data_labels4clients, distribute_data_per_client_edited\n",
    "from gan_model import Discriminator, Generator, initialize_weights\n",
    "from network import Server, Worker\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import Logger\n",
    "from fid_score import *\n",
    "from inception import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "dev = torch.device(dev)\n",
    "\n",
    "if dev == \"cuda:0\":\n",
    "    cuda = True\n",
    "else:\n",
    "    cuda = False\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 2\n",
    "CLASSES_PER_USER = 2\n",
    "WORKER_OVERRIDE = True # overrides other workers weights with the chosen worker\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LEARNING_RATE_D = 2e-3\n",
    "LEARNING_RATE_G = 2e-4\n",
    "B1 = 0.5\n",
    "B2 = 0.999\n",
    "\n",
    "NOISE_DIM = 128\n",
    "FID_BATCH_SIZE = 20\n",
    "NUM_UNIQUE_USERS = NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_classes = True\n",
    "Discriminator_trainig_iterations = 1\n",
    "chosen_strategy = 3\n",
    "trainig_strategies = {0:'fed_avg', 1:'weighted_avg_most',2:'weighted_avg_least', 3:'most_forgiving', 4:'least_forgiving'}\n",
    "trainig_strategies = trainig_strategies[chosen_strategy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# trans_cifar = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.CIFAR10(root='./datasets/cifar/', train=True, download=True, transform=trans_cifar)\n",
    "dataset_test = datasets.CIFAR10(root='./datasets/cifar/', train=False, download=True, transform=trans_cifar)\n",
    "dataloader_one = torch.utils.data.DataLoader(dataset, shuffle = True,batch_size=BATCH_SIZE)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, shuffle = True,batch_size=10000)\n",
    "\n",
    "MAX_WORKER_SAMPLE = len(dataset)/NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "\n",
    "dictionary = labels4clients(num_classes,CLASSES_PER_USER,NUM_WORKERS,NUM_UNIQUE_USERS,random_seed=False)\n",
    "\n",
    "if limit_classes:\n",
    "    worker1_classes = []\n",
    "    worker1_classes.append(dataset.class_to_idx['truck'])\n",
    "    worker1_classes.append(dataset.class_to_idx['automobile'])\n",
    "    print(worker1_classes)\n",
    "\n",
    "    worker2_classes = []\n",
    "    worker2_classes.append(dataset.class_to_idx['dog'])\n",
    "    worker2_classes.append(dataset.class_to_idx['cat'])\n",
    "    print(worker2_classes)\n",
    "\n",
    "    dictionary = {}\n",
    "    for i in worker1_classes:\n",
    "        if i in dictionary:\n",
    "            dictionary[i].append(0)\n",
    "        else:\n",
    "            dictionary[i]= [0]\n",
    "    for i in worker2_classes:\n",
    "        if i in dictionary:\n",
    "            dictionary[i].append(1)\n",
    "        else:\n",
    "            dictionary[i]= [1]\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y torch torchvision torchaudio\n",
    "# !pip install --no-cache-dir -r requirements.txt\n",
    "# !pip3 install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip3 install --no-cache-dir torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataloader_test:\n",
    "    test_imgs=img[0].to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data[0])\n",
    "# print(dataset.transforms(dataset.data[0],transforms.ToTensor()))\n",
    "# print(dataset.transforms(dataset.data[0],trans_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = trans_cifar(dataset.data[0]).cpu().detach().numpy()\n",
    "print(\"transformed shape:\", transformed.shape)\n",
    "plt.figure('normalized data')\n",
    "plt.hist(transformed.ravel(), bins=50, density=False)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data.shape)\n",
    "# print(type(dataset))\n",
    "# print(dataloader_one.dataset.data.shape)\n",
    "# x,_ = dataloader_one.dataset[0]\n",
    "# print(x.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np = np.empty((dataset.data.shape[0],dataset.data.shape[3],dataset.data.shape[1],dataset.data.shape[2]))\n",
    "print(\"train datatset shape:\",x_train_normalized_np.shape)\n",
    "for i in range(len(dataset)):\n",
    "    x_train_normalized_np[i] = trans_cifar(dataset.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('normalized data')\n",
    "bin_size = 60\n",
    "plt.hist(x_train_normalized_np[:][0].ravel(),color='r', bins=bin_size, density=False)\n",
    "plt.hist(x_train_normalized_np[:][1].ravel(),color='g', bins=bin_size, density=False)\n",
    "plt.hist(x_train_normalized_np[:][2].ravel(),color='b', bins=bin_size, density=False)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.asarray(dataset.data)\n",
    "y_train = np.asarray(dataset.targets)\n",
    "x_client_list, y_client_list = distribute_data_per_client_edited(x_train_normalized_np,y_train,dictionary,CLASSES_PER_USER,random_seed = False, max_samples_per_client = MAX_WORKER_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(x_client_list):\n",
    "    print(f'worker#{i} data count: {len(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDist(y,class_list,user_num):\n",
    "#     ax = sns.countplot(x=y)\n",
    "#     ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDist(y,class_list,user_num):\n",
    "#     # ax = sns.barplot(x=class_list,y=y)\n",
    "#     ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = 0\n",
    "class_list = [i for i in range(num_classes)]\n",
    "for i in range (len(x_client_list)):\n",
    "    length = len(y_client_list[i])\n",
    "    total_data+= length\n",
    "    y_list = np.bincount(y_client_list[i],minlength=num_classes)\n",
    "    # getDist(y_list,class_list,i)\n",
    "print(\"total used data\", total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fic_model = InceptionV3().to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_server = Server(0,LEARNING_RATE_G,B1,B2,dev)\n",
    "# initialize_weights(main_server.generator)\n",
    "# initialize_weights(main_server.global_disc)\n",
    "main_server.generator.train()\n",
    "main_server.global_disc.train()\n",
    "workers = []\n",
    "workers_weights= []\n",
    "copy_workers = False\n",
    "for i in range(NUM_WORKERS):\n",
    "    worker = Worker(i,LEARNING_RATE_D,B1,B2,dev)\n",
    "    # x_clinet_list[i] = np.transpose(x_clinet_list[i],(0, 3, 1, 2))\n",
    "    if copy_workers:\n",
    "        worker.load_worker_data(x_client_list[0], y_client_list[0])\n",
    "    else:\n",
    "        worker.load_worker_data(x_client_list[i], y_client_list[i]) \n",
    "    # initialize_weights(worker.discriminator)\n",
    "    worker.discriminator.train()\n",
    "    workers.append(worker)\n",
    "    workers_weights.append(worker.discriminator.state_dict())\n",
    "    \n",
    "# summary(main_server.generator,(128,1,1))\n",
    "# summary(workers[0].discriminator,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to make all the workers the same\n",
    "# workers_weights= []\n",
    "# for worker in workers:\n",
    "#     worker.discriminator = workers[-1].discriminator\n",
    "#     workers_weights.append(worker.discriminator.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "fixed_noise = torch.randn(36, NOISE_DIM, 1, 1).to(dev) # to use for generating output images\n",
    "\n",
    "worker_loaders = []\n",
    "\n",
    "for worker in workers:\n",
    "    # print(worker.x_data.shape)\n",
    "    worker_loaders.append([])\n",
    "    for batch_id, real in enumerate(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)):\n",
    "        worker_loaders[-1].append(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in worker_loaders:\n",
    "    plt.figure('normalized data')\n",
    "    plt.hist(worker[:][1].ravel(),color='r', bins=bin_size, density=False)\n",
    "    # plt.hist(x_train_normalized_np[:][1].ravel(),color='g', bins=bin_size, density=False)\n",
    "    # plt.hist(x_train_normalized_np[:][2].ravel(),color='b', bins=bin_size, density=False)\n",
    "    plt.xlabel(\"pixel values\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(model_name='F2U',data_name='CIFAR10')\n",
    "param_list = [\"NUM_WORKERS\", \"CLASSES_PER_USER\", \"NUM_EPOCHS\", \"BATCH_SIZE\",\n",
    "\"Discriminator_trainig_iterations\",\"LEARNING_RATE_D\", \"LEARNING_RATE_G\", \"trainig_strategies\", \"WORKER_OVERRIDE\",\"limit_classes\"]\n",
    "global_params = list(globals().items()) \n",
    "logger.log_params(param_list,global_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop for F2U (trial == FALSE)\n",
    "start = 0\n",
    "end = start + NUM_EPOCHS\n",
    "end = NUM_EPOCHS\n",
    "\n",
    "soft_max = nn.Softmax(dim=0)\n",
    "worker_contribution_cum = torch.zeros(NUM_WORKERS)\n",
    "\n",
    "for epoch in range(start,end):\n",
    "    for batch_id in range(len(worker_loaders[0])):\n",
    "\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "        fake = main_server.generator(noise)\n",
    "\n",
    "        for disc_train_iter in range(Discriminator_trainig_iterations):\n",
    "            highest_loss = 0\n",
    "            lowest_loss = math.inf\n",
    "            chosen_discriminator = None\n",
    "            worker_losses = []\n",
    "            for worker_id, worker in enumerate(workers):\n",
    "                \n",
    "                current_worker_real = worker_loaders[worker_id][batch_id].float().to(dev)\n",
    "\n",
    "                # print('worker ({}) datasum:'.format(worker_id),sum(current_worker_real.flatten()).item())\n",
    "                # print(current_worker_real.shape)\n",
    "\n",
    "                worker.d_optimizer.zero_grad()\n",
    "            \n",
    "                ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "                current_disc_real = worker.discriminator(current_worker_real).reshape(-1)\n",
    "                worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "                # print('real_classification:', round(sum(current_disc_real).item(),6),'real_loss:',round(worker.loss_disc_real.item(),6))\n",
    "                current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "                worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "                # print('fake_classification:', round(sum(current_disc_fake).item(),6),'fake_loss:',round(worker.loss_disc_fake.item(),6))\n",
    "                worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "                \n",
    "                worker.loss_disc.backward()\n",
    "                worker.d_optimizer.step()\n",
    "\n",
    "                workers_weights[worker_id] = worker.discriminator.state_dict()\n",
    "                worker_losses.append(worker.loss_disc_fake.detach())\n",
    "                # print(worker.loss_disc_fake, i)\n",
    "                if highest_loss < worker.loss_disc_fake and trainig_strategies == 'most_forgiving':\n",
    "                    highest_loss = worker.loss_disc_fake\n",
    "                    chosen_discriminator = worker_id\n",
    "                    chosen_worker = workers[chosen_discriminator]\n",
    "                    \n",
    "                    \n",
    "\n",
    "                if lowest_loss > worker.loss_disc_fake and trainig_strategies == 'least_forgiving' :\n",
    "                    lowest_loss = worker.loss_disc_fake\n",
    "                    chosen_discriminator = worker_id\n",
    "                    chosen_worker = workers[chosen_discriminator]\n",
    "                   \n",
    "            # print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item():.4f}\")\n",
    "        \n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        if trainig_strategies in ['fed_avg', 'weighted_avg_most']:\n",
    "            worker_losses = torch.FloatTensor(worker_losses)\n",
    "            # print(f\"worker_losses before: {worker_losses}\")\n",
    "            worker_losses_sm = soft_max(worker_losses)\n",
    "            # print(worker_losses_sm)\n",
    "            worker_contribution_cum += worker_losses_sm\n",
    "        elif trainig_strategies == 'weighted_avg_least':\n",
    "            # print(f\"worker_losses before: {worker_losses}\")\n",
    "            worker_losses = -1*torch.FloatTensor(worker_losses)\n",
    "            # print(f\"worker_losses after: {worker_losses}\")\n",
    "            worker_losses_sm = soft_max(worker_losses)\n",
    "            # print(f\"worker_losses_sm: {worker_losses_sm}\")\n",
    "            worker_contribution_cum += worker_losses_sm\n",
    "\n",
    "        # print(f\"worker_losses after softmax: {worker_losses_sm}\")\n",
    "\n",
    "        # worker_total_weights1 = []\n",
    "        # for worker in workers:\n",
    "        #     weight_sum = 0\n",
    "        #     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "        #         output = sum(p.detach().cpu().numpy().flatten())\n",
    "        #         weight_sum += output\n",
    "        #     worker_total_weights1.append(round(weight_sum,1))\n",
    "\n",
    "        main_server.g_optimizer.zero_grad()\n",
    "\n",
    "        if trainig_strategies == 'fed_avg':\n",
    "            avg_w = main_server.fed_average(workers_weights)\n",
    "            main_server.global_disc.load_state_dict(avg_w)\n",
    "            for worker in workers:\n",
    "                worker.discriminator.load_state_dict(avg_w)\n",
    "            output = main_server.global_disc(fake).reshape(-1)\n",
    "            chosen_worker = workers[0]\n",
    "        elif trainig_strategies in ['weighted_avg_most','weighted_avg_least']:\n",
    "            avg_w = main_server.weighted_fed_average(workers_weights,worker_losses_sm)\n",
    "            main_server.global_disc.load_state_dict(avg_w)\n",
    "            for worker in workers:\n",
    "                worker.discriminator.load_state_dict(avg_w)\n",
    "            output = main_server.global_disc(fake).reshape(-1)\n",
    "            chosen_worker = workers[0]\n",
    "        else:\n",
    "            worker_contribution_cum[chosen_discriminator]+=1\n",
    "            if WORKER_OVERRIDE:\n",
    "                for worker in workers:\n",
    "                    worker.discriminator.load_state_dict(chosen_worker.discriminator.state_dict())\n",
    "            output = chosen_worker.discriminator(fake).reshape(-1)\n",
    "        main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "        \n",
    "        main_server.loss_gen.backward()\n",
    "        # check weights of all workers before and after\n",
    "\n",
    "        main_server.g_optimizer.step()\n",
    "\n",
    "        # worker_total_weights2 = []\n",
    "        # for worker in workers:\n",
    "        #     weight_sum = 0\n",
    "        #     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "        #         output = sum(p.detach().cpu().numpy().flatten())\n",
    "        #         weight_sum += output\n",
    "        #     worker_total_weights2.append(round(weight_sum,1))\n",
    "        \n",
    "        # diff = []\n",
    "        # for i, curr_weight in enumerate(worker_total_weights2):\n",
    "        #     diff.append(abs(curr_weight-worker_total_weights1[i]))\n",
    "        # print(\"before g_optimizer:\", worker_total_weights1, \"after g_optimizer:\", worker_total_weights2, \"diff:\", diff)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logger.log_workers(workers,epoch,batch_id,len(worker_loaders[0]))\n",
    "            logger.log(chosen_worker.loss_disc.item(),main_server.loss_gen.item(),chosen_worker.loss_disc_real, chosen_worker.loss_disc_fake,epoch,batch_id,len(worker_loaders[0]))\n",
    "        # Print loss\n",
    "        if batch_id % 100 == 0:\n",
    "            fid_z = torch.randn(FID_BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "            gen_imgs = main_server.generator(fid_z.detach())\n",
    "            mu_gen, sigma_gen = calculate_activation_statistics(gen_imgs, fic_model, batch_size=FID_BATCH_SIZE,cuda=cuda,verbose=False)\n",
    "            mu_test, sigma_test = calculate_activation_statistics(test_imgs[:FID_BATCH_SIZE], fic_model, batch_size=FID_BATCH_SIZE,cuda=cuda,verbose=False)\n",
    "            fid = calculate_frechet_distance(mu_gen, sigma_gen, mu_test, sigma_test)\n",
    "            logger.log_fid(fid,epoch,batch_id,len(worker_loaders[0]))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(worker_loaders[0])} \\\n",
    "                Loss D: {chosen_worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}, FID Score: {fid:.1f}\"\n",
    "            )\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        main_server.generator.eval()\n",
    "        fake = main_server.generator(fixed_noise)\n",
    "        main_server.generator.train()\n",
    "        logger.log_images(fake,len(fake), epoch, batch_id, len(worker_loaders[0]))\n",
    "\n",
    "    chosen_w_np = worker_contribution_cum.detach().cpu().numpy()\n",
    "    ra = range(1,len(chosen_w_np)+1)\n",
    "    plt.bar(ra,chosen_w_np)\n",
    "    plt.xticks(ra)\n",
    "    plt.xlabel('worker number')\n",
    "    plt.ylabel('contribution')\n",
    "    plt.savefig('{}/worker_cont.png'.format(logger.writer.logdir))\n",
    "    plt.show()\n",
    "\n",
    "    if (epoch+1) % 50 == 0 and epoch != 0:\n",
    "        logger.save_models(main_server,workers,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(range(len(worker_chosen_counter)),worker_chosen_counter)\n",
    "# plt.xlabel('worker number')\n",
    "# plt.ylabel('chosen counter')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing the total weights\n",
    "# worker_total_weights1 = []\n",
    "# for worker in workers:\n",
    "#     weight_sum = 0\n",
    "#     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "#         output = sum(p.detach().cpu().numpy().flatten())\n",
    "#         weight_sum += output\n",
    "#     worker_total_weights1.append(round(weight_sum,1))\n",
    "\n",
    "# worker_total_weights2 = []\n",
    "# for worker in workers:\n",
    "#     weight_sum = 0\n",
    "#     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "#         output = sum(p.detach().cpu().numpy().flatten())\n",
    "#         weight_sum += output\n",
    "#     worker_total_weights2.append(round(weight_sum,1))\n",
    "\n",
    "# diff = []\n",
    "# for i, curr_weight in enumerate(worker_total_weights2):\n",
    "#     diff.append(abs(curr_weight-worker_total_weights1[i]))\n",
    "# print(\"before g_optimizer:\", worker_total_weights1, \"after g_optimizer:\", worker_total_weights2, \"diff:\", diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GAN archicture trial (trial == TRUE)\n",
    "# if trial:\n",
    "#     start = 0\n",
    "#     end = start + NUM_EPOCHS\n",
    "#     for epoch in range(start,end):\n",
    "#         for i, data in enumerate(dataloader_one):\n",
    "#             worker = workers[0]\n",
    "#             noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "#             fake = main_server.generator(noise)\n",
    "#             real, _ = data\n",
    "\n",
    "#             ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            \n",
    "#             current_disc_real = worker.discriminator(real).reshape(-1)\n",
    "#             # print('current discriminator real output', current_disc_real)\n",
    "#             worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "#             # print('worker loss_disc_real output', current_disc_real)\n",
    "#             current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "#             worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "#             worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "#             worker.discriminator.zero_grad()\n",
    "#             worker.loss_disc.backward()\n",
    "#             # total_norm_d =0\n",
    "#             # for p in list(filter(lambda p: p.grad is not None, worker.discriminator.parameters())):\n",
    "#             #     total_norm_d += p.grad.detach().data.norm(2).item()** 2\n",
    "#             # total_norm_d = total_norm_d ** 0.5\n",
    "\n",
    "#             worker.d_optimizer.step()\n",
    "\n",
    "#             ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "            \n",
    "#             output = worker.discriminator(fake).reshape(-1)\n",
    "#             main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "#             main_server.generator.zero_grad()\n",
    "#             main_server.loss_gen.backward()\n",
    "\n",
    "#             # total_norm_g =0\n",
    "#             # for p in list(filter(lambda p: p.grad is not None, main_server.generator.parameters())):\n",
    "#             #     total_norm_g += p.grad.detach().data.norm(2).item()** 2\n",
    "#             # total_norm_g = total_norm_g ** 0.5\n",
    "\n",
    "#             main_server.g_optimizer.step()\n",
    "\n",
    "\n",
    "#             logger.log(worker.loss_disc.item(),main_server.loss_gen.item(),worker.loss_disc_real, worker.loss_disc_fake,epoch,i,len(dataloader_one))\n",
    "\n",
    "#             # Print loss\n",
    "#             if i % 100 == 0:    \n",
    "#                 fid_z = torch.randn(FID_BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "#                 gen_imgs = main_server.generator(fid_z.detach())\n",
    "#                 mu_gen, sigma_gen = calculate_activation_statistics(gen_imgs, fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "#                 mu_test, sigma_test = calculate_activation_statistics(test_imgs[:FID_BATCH_SIZE], fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "#                 fid = calculate_frechet_distance(mu_gen, sigma_gen, mu_test, sigma_test)\n",
    "#                 logger.log_fid(fid,epoch,i,len(dataloader_one))\n",
    "\n",
    "#                 print(\n",
    "#                     f\"Epoch [{epoch}/{end}] Batch {i}/{len(dataloader_one)} \\\n",
    "#                     Loss D: {worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}, FID Score: {fid:.1f}\"\n",
    "#                 )\n",
    "\n",
    "#             if i% 500 == 0:\n",
    "#                 with torch.no_grad():\n",
    "#                     fake = main_server.generator(fixed_noise)\n",
    "#                     logger.log_images(fake,len(fake), epoch, i, len(dataloader_one))\n",
    "#         if epoch % 50 == 0 and epoch !=0:\n",
    "#             logger.save_models(main_server,workers,epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
