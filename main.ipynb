{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from dataTransformation import labels4clients, distribute_data_labels4clients, distribute_data_per_client_edited\n",
    "from gan_model import Discriminator, Generator, initialize_weights\n",
    "from network import Server, Worker\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import Logger\n",
    "from fid_score import *\n",
    "from inception import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "dev = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 5\n",
    "CLASSES_PER_USER = 2\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LEARNING_RATE = 2e-4\n",
    "B1 = 0.5\n",
    "B2 = 0.999\n",
    "\n",
    "\n",
    "NOISE_DIM = 128\n",
    "FID_BATCH_SIZE = 20\n",
    "NUM_UNIQUE_USERS = NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "dictionary = labels4clients(num_classes,CLASSES_PER_USER,NUM_WORKERS,NUM_UNIQUE_USERS,random_seed=False)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# trans_cifar = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.CIFAR10(root='./datasets/cifar/', train=True, download=True, transform=trans_cifar)\n",
    "dataset_test = datasets.CIFAR10(root='./datasets/cifar/', train=False, download=True, transform=trans_cifar)\n",
    "dataloader_one = torch.utils.data.DataLoader(dataset, shuffle = True,batch_size=BATCH_SIZE)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, shuffle = True,batch_size=10000)\n",
    "\n",
    "MAX_WORKER_SAMPLE = len(dataset)/NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataloader_test:\n",
    "    test_imgs=img[0].to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data[0])\n",
    "# print(dataset.transforms(dataset.data[0],transforms.ToTensor()))\n",
    "# print(dataset.transforms(dataset.data[0],trans_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = trans_cifar(dataset.data[0]).cpu().detach().numpy()\n",
    "print(\"transformed shape:\", transformed.shape)\n",
    "plt.figure('normalized data')\n",
    "plt.hist(transformed.ravel(), bins=50, density=False)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.data.shape)\n",
    "# print(type(dataset))\n",
    "# print(dataloader_one.dataset.data.shape)\n",
    "# x,_ = dataloader_one.dataset[0]\n",
    "# print(x.shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np = np.empty((dataset.data.shape[0],dataset.data.shape[3],dataset.data.shape[1],dataset.data.shape[2]))\n",
    "print(\"train datatset shape:\",x_train_normalized_np.shape)\n",
    "for i in range(len(dataset)):\n",
    "    x_train_normalized_np[i] = trans_cifar(dataset.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized_np[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('normalized data')\n",
    "bin_size = 60\n",
    "plt.hist(x_train_normalized_np[:][0].ravel(),color='r', bins=bin_size, density=False)\n",
    "plt.hist(x_train_normalized_np[:][1].ravel(),color='g', bins=bin_size, density=False)\n",
    "plt.hist(x_train_normalized_np[:][2].ravel(),color='b', bins=bin_size, density=False)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.asarray(dataset.data)\n",
    "y_train = np.asarray(dataset.targets)\n",
    "x_clinet_list, y_client_list = distribute_data_per_client_edited(x_train_normalized_np,y_train,dictionary,CLASSES_PER_USER,random_seed = False, max_samples_per_client = MAX_WORKER_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDist(y,class_list,user_num):\n",
    "#     ax = sns.countplot(x=y)\n",
    "#     ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDist(y,class_list,user_num):\n",
    "    ax = sns.barplot(x=class_list,y=y)\n",
    "    ax.set(title=\"Count of data classes for %s\" %user_num)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = 0\n",
    "class_list = [i for i in range(num_classes)]\n",
    "for i in range (len(x_clinet_list)):\n",
    "    length = len(y_client_list[i])\n",
    "    total_data+= length\n",
    "    y_list = np.bincount(y_client_list[i],minlength=num_classes)\n",
    "    getDist(y_list,class_list,i)\n",
    "print(\"total used data\", total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fic_model = InceptionV3().to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_server = Server(0,LEARNING_RATE,B1,B2,dev)\n",
    "# initialize_weights(main_server.generator)\n",
    "# initialize_weights(main_server.global_disc)\n",
    "main_server.generator.train()\n",
    "main_server.global_disc.train()\n",
    "workers = []\n",
    "workers_weights= []\n",
    "for i in range(NUM_WORKERS):\n",
    "    worker = Worker(i,LEARNING_RATE,B1,B2,dev)\n",
    "    # x_clinet_list[i] = np.transpose(x_clinet_list[i],(0, 3, 1, 2))\n",
    "    worker.load_worker_data(x_clinet_list[i], y_client_list[i]) \n",
    "    # initialize_weights(worker.discriminator)\n",
    "    worker.discriminator.train()\n",
    "    workers.append(worker)\n",
    "    workers_weights.append(worker.discriminator.state_dict())\n",
    "    \n",
    "# summary(main_server.generator,(128,1,1))\n",
    "# summary(workers[0].discriminator,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to make all the workers the same\n",
    "# workers_weights= []\n",
    "# for worker in workers:\n",
    "#     worker.discriminator = workers[-1].discriminator\n",
    "#     workers_weights.append(worker.discriminator.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "fixed_noise = torch.randn(36, NOISE_DIM, 1, 1).to(dev) # to use for generating output images\n",
    "\n",
    "worker_loaders = []\n",
    "\n",
    "for worker in workers:\n",
    "    # print(worker.x_data.shape)\n",
    "    worker_loaders.append([])\n",
    "    for batch_id, real in enumerate(DataLoader(dataset=worker.x_data,batch_size=BATCH_SIZE)):\n",
    "        worker_loaders[-1].append(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in worker_loaders:\n",
    "    plt.figure('normalized data')\n",
    "    plt.hist(worker[:][1].ravel(),color='r', bins=bin_size, density=False)\n",
    "    # plt.hist(x_train_normalized_np[:][1].ravel(),color='g', bins=bin_size, density=False)\n",
    "    # plt.hist(x_train_normalized_np[:][2].ravel(),color='b', bins=bin_size, density=False)\n",
    "    plt.xlabel(\"pixel values\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(model_name='F2U',data_name='CIFAR10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN archicture trial (trial == TRUE)\n",
    "if trial:\n",
    "    start = 0\n",
    "    end = start + NUM_EPOCHS\n",
    "    for epoch in range(start,end):\n",
    "        for i, data in enumerate(dataloader_one):\n",
    "            worker = workers[0]\n",
    "            noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "            fake = main_server.generator(noise)\n",
    "            real, _ = data\n",
    "\n",
    "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            \n",
    "            current_disc_real = worker.discriminator(real).reshape(-1)\n",
    "            # print('current discriminator real output', current_disc_real)\n",
    "            worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "            # print('worker loss_disc_real output', current_disc_real)\n",
    "            current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "            worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "            worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "            worker.discriminator.zero_grad()\n",
    "            worker.loss_disc.backward()\n",
    "            # total_norm_d =0\n",
    "            # for p in list(filter(lambda p: p.grad is not None, worker.discriminator.parameters())):\n",
    "            #     total_norm_d += p.grad.detach().data.norm(2).item()** 2\n",
    "            # total_norm_d = total_norm_d ** 0.5\n",
    "\n",
    "            worker.d_optimizer.step()\n",
    "\n",
    "            ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "            \n",
    "            output = worker.discriminator(fake).reshape(-1)\n",
    "            main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "            main_server.generator.zero_grad()\n",
    "            main_server.loss_gen.backward()\n",
    "\n",
    "            # total_norm_g =0\n",
    "            # for p in list(filter(lambda p: p.grad is not None, main_server.generator.parameters())):\n",
    "            #     total_norm_g += p.grad.detach().data.norm(2).item()** 2\n",
    "            # total_norm_g = total_norm_g ** 0.5\n",
    "\n",
    "            main_server.g_optimizer.step()\n",
    "\n",
    "\n",
    "            logger.log(worker.loss_disc.item(),main_server.loss_gen.item(),worker.loss_disc_real, worker.loss_disc_fake,epoch,i,len(dataloader_one))\n",
    "\n",
    "            # Print loss\n",
    "            if i % 100 == 0:    \n",
    "                fid_z = torch.randn(FID_BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "                gen_imgs = main_server.generator(fid_z.detach())\n",
    "                mu_gen, sigma_gen = calculate_activation_statistics(gen_imgs, fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "                mu_test, sigma_test = calculate_activation_statistics(test_imgs[:FID_BATCH_SIZE], fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "                fid = calculate_frechet_distance(mu_gen, sigma_gen, mu_test, sigma_test)\n",
    "                logger.log_fid(fid,epoch,i,len(dataloader_one))\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{end}] Batch {i}/{len(dataloader_one)} \\\n",
    "                    Loss D: {worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}, FID Score: {fid:.1f}\"\n",
    "                )\n",
    "\n",
    "            if i% 500 == 0:\n",
    "                with torch.no_grad():\n",
    "                    fake = main_server.generator(fixed_noise)\n",
    "                    logger.log_images(fake,len(fake), epoch, i, len(dataloader_one))\n",
    "        if epoch % 50 == 0 and epoch !=0:\n",
    "            logger.save_models(main_server,workers,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bshqga\\OneDrive - Scania CV\\code\\F2U\\main.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# worker_total_weights2 = []\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# for worker in workers:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m#     weight_sum = 0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m#     diff.append(abs(curr_weight-worker_total_weights1[i]))\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# print(\"before g_optimizer:\", worker_total_weights1, \"after g_optimizer:\", worker_total_weights2, \"diff:\", diff)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     logger\u001b[39m.\u001b[39;49mlog_workers(workers,epoch,batch_id,\u001b[39mlen\u001b[39;49m(worker_loaders[\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     logger\u001b[39m.\u001b[39mlog(chosen_worker\u001b[39m.\u001b[39mloss_disc\u001b[39m.\u001b[39mitem(),main_server\u001b[39m.\u001b[39mloss_gen\u001b[39m.\u001b[39mitem(),chosen_worker\u001b[39m.\u001b[39mloss_disc_real, chosen_worker\u001b[39m.\u001b[39mloss_disc_fake,epoch,batch_id,\u001b[39mlen\u001b[39m(worker_loaders[\u001b[39m0\u001b[39m]))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bshqga/OneDrive%20-%20Scania%20CV/code/F2U/main.ipynb#X35sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39m# Print loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bshqga\\OneDrive - Scania CV\\code\\F2U\\utils.py:61\u001b[0m, in \u001b[0;36mLogger.log_workers\u001b[1;34m(self, workers, epoch, n_batch, num_batches)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m i, worker \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(workers):\n\u001b[0;32m     60\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mworker_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i)\n\u001b[1;32m---> 61\u001b[0m     dic[name]\u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mloss_disc_fake\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     63\u001b[0m \u001b[39m# w1 = workers[0].loss_disc_fake.data.cpu().numpy()\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m# w2 = workers[1].loss_disc_fake.data.cpu().numpy()\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m# w3 = workers[2].loss_disc_fake.data.cpu().numpy()\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# w4 = workers[3].loss_disc_fake.data.cpu().numpy()\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# w5 = workers[4].loss_disc_fake.data.cpu().numpy()\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalars(\n\u001b[0;32m     69\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/worker_loss/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomment), dic, step)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main training loop for F2U (trial == FALSE)\n",
    "fed_avg = False\n",
    "weighted_avg = True\n",
    "soft_max = nn.Softmax(dim=0)\n",
    "if not trial:\n",
    "    start = 0\n",
    "    end = start + NUM_EPOCHS\n",
    "    end = NUM_EPOCHS\n",
    "    worker_chosen_counter = [0 for i in range(len(workers))]\n",
    "\n",
    "    for epoch in range(start,end):\n",
    "        for batch_id in range(len(worker_loaders[0])):\n",
    "\n",
    "            highest_loss = 0\n",
    "            lowest_loss = math.inf\n",
    "            chosen_discriminator = None\n",
    "            noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "            fake = main_server.generator(noise)\n",
    "            worker_losses = []\n",
    "            for worker_id, worker in enumerate(workers):\n",
    "                current_worker_real = worker_loaders[worker_id][batch_id].float().to(dev)\n",
    "\n",
    "                # print('worker ({}) datasum:'.format(worker_id),sum(current_worker_real.flatten()).item())\n",
    "                # print(current_worker_real.shape)\n",
    "\n",
    "                worker.d_optimizer.zero_grad()\n",
    "            \n",
    "                ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "                current_disc_real = worker.discriminator(current_worker_real).reshape(-1)\n",
    "                worker.loss_disc_real = criterion(current_disc_real, torch.ones_like(current_disc_real))\n",
    "                # print('real_classification:', round(sum(current_disc_real).item(),6),'real_loss:',round(worker.loss_disc_real.item(),6))\n",
    "                current_disc_fake = worker.discriminator(fake.detach()).reshape(-1)\n",
    "                worker.loss_disc_fake = criterion(current_disc_fake, torch.zeros_like(current_disc_fake))\n",
    "                # print('fake_classification:', round(sum(current_disc_fake).item(),6),'fake_loss:',round(worker.loss_disc_fake.item(),6))\n",
    "                worker.loss_disc = (worker.loss_disc_real + worker.loss_disc_fake) / 2\n",
    "                \n",
    "                worker.loss_disc.backward()\n",
    "                if batch_id % 15 ==0:\n",
    "                    worker.d_optimizer.step()\n",
    "\n",
    "                workers_weights[worker_id] = worker.discriminator.state_dict()\n",
    "                worker_losses.append(worker.loss_disc_fake)\n",
    "                # print(worker.loss_disc_fake, i)\n",
    "                # if highest_loss < worker.loss_disc_fake:\n",
    "                #     highest_loss = worker.loss_disc_fake\n",
    "                #     chosen_discriminator = worker_id\n",
    "                if lowest_loss > worker.loss_disc_fake:\n",
    "                    lowest_loss = worker.loss_disc_fake\n",
    "                    chosen_discriminator = worker_id\n",
    "            # print(f\"chosen worker is {chosen_discriminator} with loss of: {highest_loss.item():.4f}\")\n",
    "            \n",
    "            ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "            worker_losses = torch.FloatTensor(worker_losses)\n",
    "            # print(f\"worker_losses before: {worker_losses}\")\n",
    "            worker_losses_sm = soft_max(worker_losses)\n",
    "            # print(f\"worker_losses after softmax: {worker_losses_sm}\")\n",
    "\n",
    "            chosen_worker = workers[chosen_discriminator]\n",
    "            worker_chosen_counter[chosen_discriminator]+=1\n",
    "\n",
    "            # worker_total_weights1 = []\n",
    "            # for worker in workers:\n",
    "            #     weight_sum = 0\n",
    "            #     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "            #         output = sum(p.detach().cpu().numpy().flatten())\n",
    "            #         weight_sum += output\n",
    "            #     worker_total_weights1.append(round(weight_sum,1))\n",
    "\n",
    "            main_server.g_optimizer.zero_grad()\n",
    "\n",
    "            if fed_avg:\n",
    "                avg_w = main_server.fed_average(workers_weights)\n",
    "                main_server.global_disc.load_state_dict(avg_w)\n",
    "                output = main_server.global_disc(fake).reshape(-1)\n",
    "            elif weighted_avg:\n",
    "                avg_w = main_server.weighted_fed_average(workers_weights,worker_losses_sm)\n",
    "                main_server.global_disc.load_state_dict(avg_w)\n",
    "                output = main_server.global_disc(fake).reshape(-1)\n",
    "            else:\n",
    "                output = chosen_worker.discriminator(fake).reshape(-1)\n",
    "            main_server.loss_gen = criterion(output, torch.ones_like(output))\n",
    "            \n",
    "            main_server.loss_gen.backward()\n",
    "            # check weights of all workers before and after\n",
    "\n",
    "            # if batch_id % 20 ==0:\n",
    "            main_server.g_optimizer.step()\n",
    "\n",
    "            # worker_total_weights2 = []\n",
    "            # for worker in workers:\n",
    "            #     weight_sum = 0\n",
    "            #     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "            #         output = sum(p.detach().cpu().numpy().flatten())\n",
    "            #         weight_sum += output\n",
    "            #     worker_total_weights2.append(round(weight_sum,1))\n",
    "            \n",
    "            # diff = []\n",
    "            # for i, curr_weight in enumerate(worker_total_weights2):\n",
    "            #     diff.append(abs(curr_weight-worker_total_weights1[i]))\n",
    "            # print(\"before g_optimizer:\", worker_total_weights1, \"after g_optimizer:\", worker_total_weights2, \"diff:\", diff)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logger.log_workers(workers,epoch,batch_id,len(worker_loaders[0]))\n",
    "                logger.log(chosen_worker.loss_disc.item(),main_server.loss_gen.item(),chosen_worker.loss_disc_real, chosen_worker.loss_disc_fake,epoch,batch_id,len(worker_loaders[0]))\n",
    "            # Print loss\n",
    "            if batch_id % 100 == 0:\n",
    "                fid_z = torch.randn(FID_BATCH_SIZE, NOISE_DIM, 1,1).to(dev)\n",
    "                gen_imgs = main_server.generator(fid_z.detach())\n",
    "                mu_gen, sigma_gen = calculate_activation_statistics(gen_imgs, fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "                mu_test, sigma_test = calculate_activation_statistics(test_imgs[:FID_BATCH_SIZE], fic_model, batch_size=FID_BATCH_SIZE,cuda=True)\n",
    "                fid = calculate_frechet_distance(mu_gen, sigma_gen, mu_test, sigma_test)\n",
    "                logger.log_fid(fid,epoch,batch_id,len(worker_loaders[0]))\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_id}/{len(worker_loaders[0])} \\\n",
    "                    Loss D: {chosen_worker.loss_disc:.4f}, loss G: {main_server.loss_gen:.4f}, FID Score: {fid:.1f}\"\n",
    "                )\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            main_server.generator.eval()\n",
    "            fake = main_server.generator(fixed_noise)\n",
    "            main_server.generator.train()\n",
    "            logger.log_images(fake,len(fake), epoch, batch_id, len(worker_loaders[0]))\n",
    "\n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            plt.bar(range(len(worker_chosen_counter)),worker_chosen_counter)\n",
    "            plt.xlabel('worker number')\n",
    "            plt.ylabel('chosen counter')\n",
    "            plt.show()\n",
    "        if epoch % 50 == 0 and epoch != 0 and not fed_avg:\n",
    "            logger.save_models(main_server,workers,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(range(len(worker_chosen_counter)),worker_chosen_counter)\n",
    "# plt.xlabel('worker number')\n",
    "# plt.ylabel('chosen counter')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing the total weights\n",
    "# worker_total_weights1 = []\n",
    "# for worker in workers:\n",
    "#     weight_sum = 0\n",
    "#     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "#         output = sum(p.detach().cpu().numpy().flatten())\n",
    "#         weight_sum += output\n",
    "#     worker_total_weights1.append(round(weight_sum,1))\n",
    "\n",
    "# worker_total_weights2 = []\n",
    "# for worker in workers:\n",
    "#     weight_sum = 0\n",
    "#     for i, p in enumerate(worker.discriminator.parameters()):\n",
    "#         output = sum(p.detach().cpu().numpy().flatten())\n",
    "#         weight_sum += output\n",
    "#     worker_total_weights2.append(round(weight_sum,1))\n",
    "\n",
    "# diff = []\n",
    "# for i, curr_weight in enumerate(worker_total_weights2):\n",
    "#     diff.append(abs(curr_weight-worker_total_weights1[i]))\n",
    "# print(\"before g_optimizer:\", worker_total_weights1, \"after g_optimizer:\", worker_total_weights2, \"diff:\", diff)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
